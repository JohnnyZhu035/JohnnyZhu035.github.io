<!doctypehtml><html class="theme-next pisces use-motion" lang=zh-Hans><meta charset=UTF-8><meta content=IE=edge http-equiv=X-UA-Compatible><meta content=width=device-width,initial-scale=1,maximum-scale=1 name=viewport><meta content=#222 name=theme-color><script src=/lib/pace/pace.min.js></script><link href=/lib/pace/.min.css?v=1.0.2 rel=stylesheet><meta content=no-transform http-equiv=Cache-Control><meta content=no-siteapp http-equiv=Cache-Control><link href=/lib/font-awesome/css/font-awesome.min.css?v=4.6.2 rel=stylesheet><link href=/css/main.css?v=6.0.0 rel=stylesheet><link href=/images/apple-touch-icon-next.png?v=6.0.0 rel=apple-touch-icon sizes=180x180><link href=/images/favicon-32x32-next.png?v=6.0.0 rel=icon sizes=32x32 type=image/png><link href=/images/favicon-16x16-next.png?v=6.0.0 rel=icon sizes=16x16 type=image/png><link color=#222 href=/images/logo.svg?v=6.0.0 rel=mask-icon><meta content="Hexo, NexT" name=keywords><meta content=website property=og:type><meta content=Reading-Paper property=og:title><meta content=https://johnnyzhu035.github.io/tags/index.html property=og:url><meta content="JohnnyZhu's HomePage!" property=og:site_name><meta property=og:locale><meta content=2024-07-18T09:23:47.000Z property=article:published_time><meta content=2024-07-18T09:38:55.907Z property=article:modified_time><meta content=Johnny property=article:author><meta content=summary name=twitter:card><script id=hexo.configurations>var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Pisces',
    version: '6.0.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };</script><link href=https://JohnnyZhu035.github.io/tags/ rel=canonical><title>JohnnyZhu's HomePage!</title><meta content="Hexo 7.3.0" name=generator><body itemscope itemtype=http://schema.org/WebPage lang=zh-Hans><div class="container sidebar-position-left page-home"><div class=headband></div><header class=header id=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-wrapper><div class=site-meta><div class=custom-logo-site-title><a class=brand href=/ rel=start> <span class=logo-line-before><i></i></span> <span class=site-title>JohnnyZhu's HomePage!</span> <span class=logo-line-after><i></i></span> </a></div><p class=site-subtitle>An undergraduate student in Southeast University Chien-Shiung Wu College</div><div class=site-nav-toggle><button><span class=btn-bar></span> <span class=btn-bar></span> <span class=btn-bar></span></button></div></div><nav class=site-nav><ul class=menu id=menu><li class="menu-item menu-item-home"><a href=/ rel=section> <i class="menu-item-icon fa fa-fw fa-home"></i> <br> 首页 </a><li class="menu-item menu-item-tags"><a href=/tags/ rel=section> <i class="menu-item-icon fa fa-fw fa-tags"></i> <br> 标签 </a><li class="menu-item menu-item-archives"><a href=/archives/ rel=section> <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br> 归档 </a><li class="menu-item menu-item-search"><a class=popup-trigger href=javascript:;> <i class="menu-item-icon fa fa-search fa-fw"></i> <br> 搜索 </a></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon> <i class="fa fa-search"></i> </span><span class=popup-btn-close> <i class="fa fa-times-circle"></i> </span><div class=local-search-input-wrapper><input autocomplete=off id=local-search-input placeholder=搜索... spellcheck=false></div></div><div id=local-search-result></div></div></div></nav></div></header><main class=main id=main><div class=main-inner><div class=content-wrap><div class=content id=content><section class=posts-expand id=posts><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/10/09/code%E2%80%9D/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/10/09/code%E2%80%9D/ itemprop=url>Rotation Matrix with MATLAB Code</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-10-09T17:10:01+08:00 title=创建于>2024-10-09</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-10-09T17:52:19+08:00 itemprop=dateModified title=更新于>2024-10-09</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/10/09/code%E2%80%9D/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/10/09/code”/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>2 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=What-we-are-up-to-do-here><a title="What we are up to do here" class=headerlink href=#What-we-are-up-to-do-here></a>What we are up to do here</h2><p><strong>In this blog,we will visualize how rotation matrix works</strong><br>Code first:<br><figure class="highlight plaintext"><figcaption><span>[Code to demonstrate rotation]</span></figcaption><table><tr><td class=gutter><pre><span class=line>1</span><br><span class=line>2</span><br><span class=line>3</span><br><span class=line>4</span><br><span class=line>5</span><br><span class=line>6</span><br><span class=line>7</span><br><span class=line>8</span><br><span class=line>9</span><br><span class=line>10</span><br><span class=line>11</span><br><span class=line>12</span><br></pre><td class=code><pre><span class=line>lb=-pi;</span><br><span class=line>ub=pi;</span><br><span class=line>theta=lb+(ub-lb)*rand(1);</span><br><span class=line>x=[1,1];</span><br><span class=line>%figure;</span><br><span class=line>%quiver(0,0,x(1),x(2));</span><br><span class=line>R=[cos(theta),sin(theta);sin(-theta),cos(theta)];</span><br><span class=line>x_trans=R*x';</span><br><span class=line>disp((theta*360)/(2*pi));</span><br><span class=line>figure;</span><br><span class=line>quiver(0,0,x_trans(1),x_trans(2));</span><br><span class=line>title('transformed angle:',(theta*360)/(2*pi));</span><br></pre></table></figure><br>With the code,we can plot figures of rotation,as follows:<br><strong><em>Demonstrate original vector</em></strong><br><img title="Original Vector" alt=图片 src=/images/OriginalVec.jpg><br><strong><em>Demonstrate rotated vector</em></strong><br><img title="Original Vector" alt=图片 src=/images/RotatedVec.jpg><br>By setting the rotation angle $\theta$ between $[-\pi,\pi]$,we can see in the example above,the vector is rotated anti-clockwise with the angle demonstrated.</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/25/Continuous-RV-distributions/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/25/Continuous-RV-distributions/ itemprop=url>Continuous RV distributions</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-25T15:05:41+08:00 title=创建于>2024-07-25</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-25T16:06:34+08:00 itemprop=dateModified title=更新于>2024-07-25</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/25/Continuous-RV-distributions/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/25/Continuous-RV-distributions/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>3 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Preface><a class=headerlink href=#Preface title=Preface></a>Preface</h2><p>In recent days of browsing the Internet for relevant learning materials,I find it good to have a better understanding of possibilities by looking at some basic concepts!Therefore,I come up with this chapter:<em>Continuous RV distributions</em>,where RV stands for random variables.<h2 id=Continuous-Random-Variables><a title="Continuous Random Variables" class=headerlink href=#Continuous-Random-Variables></a>Continuous Random Variables</h2><p>Every continuous random variable has a probability density function (PDF), instead of a probability mass function (PMF),where the limit in calculus takes place,showing in a given section,the possibility of a given event to happen is 0.<h3 id=PDF-and-CDF><a title="PDF and CDF" class=headerlink href=#PDF-and-CDF></a>PDF and CDF</h3><p>PDF:<br>define $f(x)$ as the possibility density function of $x$ so that:<blockquote><p>$\int_{-\infty}^{\infty}f(x)dx=1$<br>$\mathbb P(x\approx q)=\mathbb P(q-\frac{\epsilon}{2}\leq X\leq q+\frac{\epsilon}{2})\approx \epsilon f_x(q)$</blockquote><p>CDF:<br>define $F(x)$ as the cumulative distribution function so that:<blockquote><p>$F_x(w)=\mathbb P(x\leq w)=\int _{-\infty}^{w}{f(x)dx}$</blockquote><p>Noticably,PDF is the derivative of CDF.<h3 id=Expectation-and-Variance><a title="Expectation and Variance" class=headerlink href=#Expectation-and-Variance></a>Expectation and Variance</h3><p>The expectation of a continuous RV can be counted as follows:<blockquote><p>$\mathbb E(x)=\int_{-\infty}^{\infty}{tf_x(t)dt}$</blockquote><p>The variance of a RV can be counted as follows:<blockquote><p>$Var(x)=\mathbb E(x^2)-\mathbb E^2(x)$</blockquote><p>or also:<blockquote><p>$Var(x)=\int_{-\infty}^{\infty}{(t-\mathbb E(x))f_x(t)dt}$</blockquote><h3 id=Exponential-Family><a title="Exponential Family" class=headerlink href=#Exponential-Family></a>Exponential Family</h3><p>$X\sim Exp(\lambda)$,if X follows the pdf:<blockquote><p>$f_X(x)=\begin{cases}\lambda e^{-\lambda x},x\geq 0\\0,otherwise\end{cases}$</blockquote><p>Calculating this,we know:<blockquote><p>$\mathbb E(X)=\frac{1}{\lambda},Var(X)=\frac{1}{\lambda^2}$</blockquote><p>The cdf of it is:<blockquote><p>$F_X(x)=\begin{cases}1-e^{-\lambda x},x\geq 0\\0,otherwise\end{cases}$</blockquote><h3 id=The-Gamma-Gamma-RV><a title="The Gamma($\Gamma$) RV" class=headerlink href=#The-Gamma-Gamma-RV></a>The Gamma($\Gamma$) RV</h3><p>$X\sim Gamma(r,\lambda)$ if and only if $X$ has the following pdf:<blockquote><p>$f_X(x)=\begin{cases}\frac{\lambda^r}{(r-1)!}{x^{r-1}e^{-\lambda x}},x>0\\0,otherwise\end{cases}$</blockquote><p>where:<blockquote><p>$\mathbb E(X)=\frac{r}{\lambda},Var(X)=\frac{r}{\lambda^2}$</blockquote></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/23/MMSE-and-Relative-Methods/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/23/MMSE-and-Relative-Methods/ itemprop=url>MMSE and Relative Methods</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-23T09:43:54+08:00 title=创建于>2024-07-23</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-24T11:47:48+08:00 itemprop=dateModified title=更新于>2024-07-24</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/23/MMSE-and-Relative-Methods/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/23/MMSE-and-Relative-Methods/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>5 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=MMSE-Estimation><a title="MMSE Estimation" class=headerlink href=#MMSE-Estimation></a>MMSE Estimation</h2><h3 id=Estimation-with-PDF><a title="Estimation with PDF" class=headerlink href=#Estimation-with-PDF></a>Estimation with PDF</h3><p>For a random variable $x:\Omega \to \mathbb R^n$ with its PDF $p^x$<br>We can <em>predict</em> or <em>estimate</em> the outcome:<blockquote><p>Given <em>cost function</em> $c$: $\mathbb<br>R^n\times\mathbb R^n\to R$<br>Pick <em>estimate</em> $\hat x$ to minimize $\mathbf Ex(x,\hat x)$</blockquote><h4 id=The-Cost-Function><a title="The Cost Function" class=headerlink href=#The-Cost-Function></a>The Cost Function</h4><blockquote><p>$c(x,\hat x)=||x-\hat x||^2$</blockquote><h4 id=The-Mean-Square-Error><a title="The Mean Square Error" class=headerlink href=#The-Mean-Square-Error></a>The Mean Square Error</h4><p>The <em>mean square error</em>(MSE) is:<blockquote><p>$\mathbf E(||x-\hat x||^2)=\int ||x-\hat x||^2p^x(x)dx$</blockquote><p>The PDF in this equation can be interpreted as a kind of special weight function!<h4 id=The-Minimum-Mean-Square-Error><a title="The Minimum Mean-Square Error" class=headerlink href=#The-Minimum-Mean-Square-Error></a>The Minimum Mean-Square Error</h4><p>The <em>minimum mean-square error</em> is defined as:<blockquote><p>$\displaystyle \min_{\hat x}\mathbf E(||x-\hat x||^2)$</blockquote><p>We can derive from that formula:<blockquote><p>$\displaystyle \min_{\hat x}\mathbf E(||x-\hat x||^2)\\=\displaystyle \min_{\hat x}\mathbf E((x-\hat x)^T(x-\hat x))\\=\mathbf E||x||^2-2\hat x^T\mathbf Ex+\hat x^T \hat x$</blockquote><p>Take the derivative of it,we get:<blockquote><p>$\hat x_{mmse}=\mathbf Ex$</blockquote><p>The mean square error of it is:<blockquote><p>$\mathbf E(||x-\hat x_{mmse}||^2)=\mathbf{trace cov}(x)$</blockquote><h4 id=The-Mean-Variance-Decomposition><a title="The Mean-Variance Decomposition" class=headerlink href=#The-Mean-Variance-Decomposition></a>The Mean-Variance Decomposition</h4><p>The MVD can be interpreted as:<blockquote><p>$\mathbf E(||z||^2)=\mathbf E(||z-\mathbf Ez||^2)+||\mathbf Ez||^2$</blockquote><p>Applying this to the $z=x-\hat x$<blockquote><p>$\mathbf E(||x-\hat x||^2)=\mathbf E(||x-\mathbf Ex||^2)+||\hat x-\mathbf Ex||^2$</blockquote><p>The first term is the <em>variance</em> of $x$<br>The second term is the <em>bias</em> of $x$<h3 id=The-Estimation-Problem><a title="The Estimation Problem" class=headerlink href=#The-Estimation-Problem></a>The Estimation Problem</h3><p>$x,y$ are random variables,with joint PDF $p(x,y)$<br>We have already measured the variable $y=y_{meas}$<br>Now we want to find the MMSE of $x$ given $y=y_{meas}$<br>Note that the <em>estimator</em> stands for a function $\phi:\mathbb R^m\to \mathbb R^n$<br>We define $\hat x_{est}=\phi(y_{meas})$<br>How can we find the function $\phi$ to minimize the cost function?<blockquote><p>$J=\mathbf E(||\phi(y)-x||^2)$</blockquote><p>We define the <em>marginal PDF</em> of $y$ as:<blockquote><p>$p^y(y)=\int p(x,y)dx$<blockquote><p>Wow!We can easily notice that this is a continuous form of the marginal function!(See in the factor graph section!).What a surprise!</blockquote></blockquote><p>And the <em>conditioned pdf</em> of $y$:<blockquote><p>$p^{|y}(y)=\frac{p(x,y)}{p^y(y)}$</blockquote><h3 id=The-MMSE-Estimator><a title="The MMSE Estimator" class=headerlink href=#The-MMSE-Estimator></a>The MMSE Estimator</h3><p>The <em>mean-square-error conditioned on</em> $y$ is $e_{cond}(y)$,given by:<blockquote><p>$e_{cond}(y)=\int||\phi(y)-x||^2p^{|y}(x,y)dx$</blockquote><p>Then the mean square error $J$ is given by:<blockquote><p>$J=\mathbf E(e_{cond}(y))$</blockquote><p>Because:<blockquote><p>$J=\int\int ||\phi(y)-x||^2p(x,y)dxdy\\=\int p^y(y)e_{cond}(y)dy$</blockquote><h3 id=The-affine-transformation-in-signal-processing><a title="The affine transformation(in signal processing)" class=headerlink href=#The-affine-transformation-in-signal-processing></a>The affine transformation(in signal processing)</h3><p>For a signal $\mathbf y$ received and a certain <em>channel matrix</em> $\mathbf H$,we have:<blockquote><p>$\mathbf {y=Hs+n}$</blockquote><p>where we suppose $\mathbf{y\in R^{N_r},s\in R^{N_t}},H\in R^{N_r\times N_t}$<br>The expectation of $\mathbf y$ is:<blockquote><p>$\mathbf {Ey=E(Hs+n)=HE(\mathbf s)}=H\mu$</blockquote><p>The covariance is:<blockquote><p>$\mathbf {\Sigma_y =E(y-\hat y)(y-\hat y)^T=EH(s- \hat s)(s-\hat s)^TH^T}\\$<br>$=\mathbf{H\Sigma_sH^T+\Sigma_n}$</blockquote><p>which denotes the <em>signal covariance</em> and <em>noise covariance</em></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/22/Gaussian-Distribution/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/22/Gaussian-Distribution/ itemprop=url>Gaussian Distribution</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-22T16:46:46+08:00 title=创建于>2024-07-22</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-23T09:13:33+08:00 itemprop=dateModified title=更新于>2024-07-23</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/22/Gaussian-Distribution/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/22/Gaussian-Distribution/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>8 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Why-Gaussian-Distribution–Central-Limit-Theorem><a title="Why Gaussian Distribution–Central Limit Theorem" class=headerlink href=#Why-Gaussian-Distribution–Central-Limit-Theorem></a>Why Gaussian Distribution–Central Limit Theorem</h2><h3 id=Population-and-Sample><a title="Population and Sample" class=headerlink href=#Population-and-Sample></a>Population and Sample</h3><p>The <em>population</em> refers to the whole set of individuals that we want to study,and the <em>sample</em> refers to the subset that we choose from the population to study(in respect for the fact that we can’t choose the whole population to study in a row)<h3 id=Central-Limit-Theorem><a title="Central Limit Theorem" class=headerlink href=#Central-Limit-Theorem></a>Central Limit Theorem</h3><p>The Central Limit Theorem (CLT) is a fundamental theorem in probability theory and statistics that states that the distribution of the sum (or average) of a large number of independent, identically distributed variables will be approximately normal, regardless of the underlying distribution.<p>More specifically, if we take a sample of $n$ observations from any population, calculate the sample’s mean, and repeat that process a large number of times, the distribution of those sample means will be normally distributed. This distribution will have the same mean as the population from which the samples were drawn, and its standard deviation (also known as the standard error) will be equal to the standard deviation of the population divided by the square root of the sample size.<h2 id=Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution><a title="Single Variable(Univariate) Gaussian Distribution(Normal Distribution)" class=headerlink href=#Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution></a>Single Variable(Univariate) Gaussian Distribution(Normal Distribution)</h2><h3 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h3><p>For a random variable $x$,if its <em>probability density function</em> is:<blockquote><p>$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$</blockquote><p>where $\mu$ and $\sigma$ are the mean and variance of $x$.Then we can say the variable follows the normal distribution.<br>If the variable $x$ follows the Gaussian distribution,we can denote this kind of distribution as:<blockquote><p>$x\sim N(\mu,\sigma^2)$</blockquote><h2 id=Multivariate-Normal-Distribution><a title="Multivariate Normal Distribution" class=headerlink href=#Multivariate-Normal-Distribution></a>Multivariate Normal Distribution</h2><p>For a mutivariable $n\times 1$ vector $X$ distributed following the multivariate distribution,with population mean vector $\mathbf{\mu}$ and a variance-covariance matrix $\Sigma$,the PDF(possibbility density function) can be described as:<blockquote><p>$\phi(X)=\frac{|\Sigma|^{\frac{-1}{2}}}{(2\pi)^{\frac{n}{2}}}\exp{\frac{-1}{2}(X-\mathbf \mu)^T\Sigma^{-1}(X-\mathbf\mu) }$</blockquote><p>If $n=2$ ,then we get the <em>bivariate normal distribution</em>,resulting in a bell-shaped curve in three dimensions.<h3 id=Mahalanobis-Distance><a title="Mahalanobis Distance" class=headerlink href=#Mahalanobis-Distance></a>Mahalanobis Distance</h3><p>Noticably,the mahalanobis distance is a distance!In the univariate case,if it is greater than zero,then the variable is larger than mean,vice versa.<br>In the multivariate case,since the covariance matrix $\Sigma$ is always positive definite,the distance is always greater than 0!<h4 id=In-Univariate-Normal-Distribution><a title="In Univariate Normal Distribution" class=headerlink href=#In-Univariate-Normal-Distribution></a>In Univariate Normal Distribution</h4><p>This distance can be measured as:<blockquote><p>$d=\frac{x-\mu}{\sigma}$</blockquote><h4 id=In-Multivariate-Normal-Distribution><a title="In Multivariate Normal Distribution" class=headerlink href=#In-Multivariate-Normal-Distribution></a>In Multivariate Normal Distribution</h4><p>Also,this distance can be measured as:<blockquote><p>$d=\sqrt{(X-\mu)^T\Sigma^{-1}(X-\mu)}$</blockquote><h3 id=Integral-Property><a title="Integral Property" class=headerlink href=#Integral-Property></a>Integral Property</h3><p>For univariate normal distribution:<blockquote><p>$\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}{\exp{\frac{-1}{2\sigma^2}(x-\mu)^2}}=1$</blockquote><p>For multivariate normal distribution:<blockquote><p>$\frac{1}{2\pi^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}…\int_{-\infty}^{\infty}\exp{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)dx_1dx_2…dx_n}=1$</blockquote><h2 id=Diagonal-Covariance-Matrix-Product-Gaussian><a title="Diagonal Covariance Matrix->Product Gaussian" class=headerlink href=#Diagonal-Covariance-Matrix-Product-Gaussian></a>Diagonal Covariance Matrix->Product Gaussian</h2><p>Let’s imagine a diagonal covariance matrix $\Sigma$,OK,cool!This means the variables are independent!<br>Let’s take a $2\times 2$ covariance matrix for example:<br>In this column,we define $X=(x_1,x_2)$,the variance matrix $\Sigma = [\begin{matrix}<br>\sigma_1^2 & 0 \\<br>0 & \sigma_2^2<br>\end{matrix} ]$<br>and the mean vector $\mu=(\mu_1,\mu_2)$.<br>WE can easily compute the inverse matrix $\Sigma^{-1}=[\begin{matrix}<br>\frac{1}{\sigma_1^2} & 0 \\<br>0 & \frac{1}{\sigma_2^2}<br>\end{matrix} ]$<br>In this case,we can find the PDF as:<blockquote><p>$p(x;\mu,\Sigma)=\frac{1}{2\pi\sigma_1\sigma_2}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}$</blockquote><p>Further computing it,we get:<blockquote><p>$p(x;\mu,\Sigma)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp{(-\frac{1}{2\sigma_1^2}(x_1-\mu_1)^2)}\frac{1}{\sqrt{2\pi}\sigma_2}\exp{(-\frac{1}{2\sigma_2^2}(x_2-\mu_2)^2)}$</blockquote><p>Further corollary shows for a n-dimensional variable vector $x$,we have:<br>$p(x;\mu,\Sigma)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_{i}}\exp{(-\frac{1}{2\sigma_i^2}(x_i-\mu_i)^2)}$<h2 id=Ellipse-Where-Does-This-Thing-Come-From><a title="Ellipse:Where Does This Thing Come From??" class=headerlink href=#Ellipse-Where-Does-This-Thing-Come-From></a>Ellipse:Where Does This Thing Come From??</h2><h3 id=Isocontour-Hooray-Geography><a title="Isocontour(Hooray Geography!)" class=headerlink href=#Isocontour-Hooray-Geography></a>Isocontour(Hooray Geography!)</h3><p>Let us take the same definition above and consider this:<blockquote><p>Let $c=\frac{1}{2\pi\sigma_1\sigma_2}\exp(-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2)$<br>$2\pi c\sigma_1\sigma_2=\exp(-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2)$<br>$\log(2\pi\sigma_1\sigma_2)=-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2$<br>$1=\frac{(x_1-\mu_1)^2}{2\sigma_1^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}+\frac{(x_2-\mu_2)^2}{2\sigma_2^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$</blockquote><p>Define:<blockquote><p>$r_1=\sqrt{2\sigma_1^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$<br>$r_2=\sqrt{2\sigma_2^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$</blockquote><p>An ellipse occurs!<h3 id=Non-diagonal-Higher-Dimensional-–-Still-Symmetric><a title="Non-diagonal/Higher Dimensional – Still Symmetric" class=headerlink href=#Non-diagonal-Higher-Dimensional-–-Still-Symmetric></a>Non-diagonal/Higher Dimensional – Still Symmetric</h3><p>For a two dimensional covariance matrix,this turns to a non-axis-aligned ellipse.<br>For a higher dimenstional one,say in the $n$-dimensional case,we get a high dimensional ellipsoids in $\mathbf R^n$<h2 id=Linear-Transformation-This-Is-a-Matrix><a title="Linear Transformation: This Is a Matrix!" class=headerlink href=#Linear-Transformation-This-Is-a-Matrix></a>Linear Transformation: This Is a Matrix!</h2><p>Here is a theorem saying:<blockquote><p>Let $X\sim \mathcal N(\mu,\Sigma)$ for some $\mu\in \mathbf R^n$ and $\Sigma\in \mathbf S_{++}^n$.Then there exists a matrix $B\in R^{n\times n}$ such that if $Z=B^{-1}(X-\mu)$,then $Z\sim \mathcal N(0,I)$</blockquote></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/22/Variance-and-Covariance/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/22/Variance-and-Covariance/ itemprop=url>Variance and Covariance</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-22T14:38:20+08:00 title=创建于>2024-07-22</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T16:06:02+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/22/Variance-and-Covariance/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/22/Variance-and-Covariance/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>2 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Variance><a class=headerlink href=#Variance title=Variance></a>Variance</h2><p>The <em>variance</em> of a random variable $X$ can be defined as:<blockquote><p>If $\mathbb E(X)=\mu$</blockquote><blockquote><p>$var(X)=\mathbb E((x-\mu)^2)$</blockquote><p>For its <em>standard deviation</em>,soometimes denotes as $sd(X)$,is defined as<blockquote><p>$sd(X)=\sqrt{var(X)}$</blockquote><p>The variance has the following properties:<blockquote><p>$var(X+C)=var(X)$<br>$var(bX)=b^2var(X)$</blockquote><p>Similarly,in the covariance matrix chapter,the <em>covariance matrix</em> has near-the-same properties.<h3 id=The-Tchebychev-inequality><a title="The Tchebychev inequality" class=headerlink href=#The-Tchebychev-inequality></a>The Tchebychev inequality</h3><blockquote><p>$\mathbb P{(|X-\mu|)>\epsilon}\leq \frac{var(X)}{\epsilon^2}$</blockquote><p>where $\mu$ is the expectation of $X$,and $\epsilon>0$<br>The variance can be sometimes canculated in another way:<blockquote><p>$var(X)=\mathbb E(X^2)-\mathbb E(X)^2$</blockquote><h2 id=Covariance><a class=headerlink href=#Covariance title=Covariance></a>Covariance</h2><p>The <em>covariance</em> of two random variables $X$ and $Y$ is defined as:<blockquote><p>$cov(X,Y)=\mathbb E{(X-\mathbb E(X))(Y-\mathbb E(Y))}$</blockquote><p>for a more complicated expression with $a,b,c,d$ as constants and $U,V,X,Y$ as random variables,we denote:<blockquote><p>$cov(aU+bV,cX+dY)=accov(U,X)+bccov(V,X)+adcov(U,Y)+bdcov(V,Y)$</blockquote><h2 id=Correlation><a class=headerlink href=#Correlation title=Correlation></a>Correlation</h2><p>The <em>correlation</em> between two variables $Y$ and $Z$ can be denoted as:<blockquote><p>corr(Y,Z)=\frac{cov(Y,Z)}{\sqrt{var(Y)var(Z)}}</blockquote><h2 id=Standardized-Variables><a title="Standardized Variables" class=headerlink href=#Standardized-Variables></a>Standardized Variables</h2><p>To standardize between different variables,we need to find a way:<em>standardized variables</em>.For two variables $Y$ and $Z$,with their mean and variance $\mu _Y$ $\sigma^2_Y$ $\mu _Z$ $\sigma^2_Z$.We standardize each with the following:<blockquote><p>$Y’=\frac{Y-\mu_Y}{\sigma_Y}$<br>$Z’=\frac{Z-\mu_Z}{\sigma_Z}$</blockquote><p>In this way,the mean of $Y’,Z’$ is 0 and the variance of them is 1,and<blockquote><p>$corr(Y,Z)=cov(Y’,Z’)=\mathbb E(Y’Z’)$</blockquote></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=url>Analysis:How to Form a Paper</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-19T10:31:32+08:00 title=创建于>2024-07-19</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T15:16:33+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/19/Analysis-How-to-Form-a-Paper/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/19/Analysis-How-to-Form-a-Paper/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>1 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Abstract><a class=headerlink href=#Abstract title=Abstract></a><strong>Abstract</strong></h2><p>$\quad$ Recent environment encourage students to create their own works or even present paper in the undergraduate period.But for most of the first and second year student,they find themselves hard to acquire the basic and needed skills and knowledge to structurize a formal paper with great proficiency.In order to prevent students from being tortured and to prevent peer viewers and mentors from relentlessly modifying and correcting paper,this work is proposed.The aim of this paper is to propose the conventional formula of how to c</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=url>First Glance at Covariance Matrix</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-18T21:43:07+08:00 title=创建于>2024-07-18</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-19T08:49:12+08:00 itemprop=dateModified title=更新于>2024-07-19</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/18/First-Glance-at-Covariance-Matrix/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/18/First-Glance-at-Covariance-Matrix/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>3 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h2><p>Let $X$ be a $N\times 1$ random column vector.The <em>covariance matrix</em> of $X$ is denoted as $Var[X]$ ,and defined as<blockquote><p>$Var[X]=E[(X-E(X))(X-E[X])^T]$</blockquote><p>where $E(\cdot)$ denoted the expectation of $\cdot$.<br>In the equation above,we can transform the dot product form into square form,in other words:<blockquote><p>$E[(X-E(X))(X-E[X])^T]=E[(X-E(X))^2]$</blockquote><h2 id=Example><a class=headerlink href=#Example title=Example></a>Example</h2><p>Suppose $x$ is a $2\times 1$ vector,with $X_1,X_2$ as its components.<br>Let<blockquote><p>$Var[X_1]=2$<br>$Var[X_2]=4$<br>$Cov[X_1,X_2]=1$</blockquote><p>Noticably,<blockquote><p>$Var[X]=\begin{bmatrix}<br>Var[X_1] &Cov[X_1,X_2]\\<br>Cov[X_2,X_1] &Var[X_2]<br>\end{bmatrix}\<br>=\begin{bmatrix}<br>2 &1\\<br>1 &4<br>\end{bmatrix}<br>$</blockquote><p>The covariance matrix can also be counted by using the equivalent formula below:<blockquote><p>$Var[X]=E[XX^T]-E[X]E[X]^T$</blockquote><p>Also,we have such corollaries below:<blockquote><p>$Var[a+X]=Var[X]$</blockquote><blockquote><p>$Var[bX]=bVar[X]b^T$</blockquote><blockquote><p>$Var[X]^T=Var[X]$</blockquote><h2 id=Covariance-between-Linear-Transformations><a title="Covariance between Linear Transformations" class=headerlink href=#Covariance-between-Linear-Transformations></a>Covariance between Linear Transformations</h2><p>Let $a$ and $b$ be two constant $1\times K$ vectors and $X$ a $K\times 1$ random vector.The covariance between two linear transformations $aX$ and $bX$ is<blockquote><p>$Cov[aX,bX]=aVar[X]b^T$</blockquote><h2 id=Cross-Covariance><a class=headerlink href=#Cross-Covariance title=Cross-Covariance></a>Cross-Covariance</h2><p>For two random vectors,what will happen?<br>Let $X$ be a $K\times 1$ vector and $Y$ be a $L\times 1$ vector(random).The covariance matrix between $X,Y$ is denoted by $Cov[X,Y]$<blockquote><p>$Cov[X,Y]=E[(X-E(X)(Y-E(Y))^T]$</blockquote><p>resulting in a $K\times L$ matrix.<br>Apparently,$Cov[X,Y]\neq Cov[Y,X]$,but $Cov[X,Y]= (Cov[Y,X])^T$<br>What’s worth noting is that the Covariance Matrix bears good <em>linearity</em>!<h2 id=Reference><a class=headerlink href=#Reference title=Reference:></a>Reference:</h2><p><em>Taboga, Marco (2021). “Covariance matrix”, Lectures on probability theory and mathematical statistics. Kindle Direct Publishing. Online appendix. <a href=https://www.statlect.com/fundamentals-of-probability/covariance-matrix rel=noopener target=_blank>https://www.statlect.com/fundamentals-of-probability/covariance-matrix</a>.</em></div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=url>Reading:Pattern Recognition and Machine Learning</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-17T15:11:56+08:00 title=创建于>2024-07-17</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-17T17:25:17+08:00 itemprop=dateModified title=更新于>2024-07-17</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>2 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Chapter-8-Graphical-Models><a title="Chapter 8:Graphical Models" class=headerlink href=#Chapter-8-Graphical-Models></a>Chapter 8:Graphical Models</h2><p>In graphical models,there are two parts:<blockquote><p>nodes or vertices<br>links or edges or arcs</blockquote><p>The graphical models mainly divide into two parts<blockquote><p><em>Bayesian networks</em> or <em>directed graphical models</em></blockquote><blockquote><p><em>Markov random fields</em> or <em>undirected graphical models</em></blockquote><blockquote><p>“Directed graphs are useful for expressing causal relationships between<br> random variables, whereas undirected graphs are better suited to expressing soft con<br>straints between random variables. For the purposes of solving inference problems,<br> it is often convenient to convert both directed and undirected graphs into a different<br> representation called a factor graph.”</blockquote><h3 id=Bayesian-Networks><a title="Bayesian Networks" class=headerlink href=#Bayesian-Networks></a>Bayesian Networks</h3><p><em>Multiplication Theorem of Probabilities</em><br>This can be denoted as:<blockquote><p>$p(a,b)=p(a|b)p(b)$</blockquote><p>which originates from the <em>conditional probability</em><blockquote><p>$p(A|B)=\frac{p(AB)}{p(B)}$</blockquote><p>For this relationship $P(a,b,c)$,we can take it separately as:<blockquote><p>$P(a,b,c)=P(c|a,b)P(a,b)=P(c|a,b)P(b|a)P(a)$</blockquote><p>This can be represented by a graph:<br><img title="Graph Denoting a,b,c" alt=图片 src=/images/graph81.png><br>Now let’s consider a more complicated scenario,there are N numbers in a column vector, polynomial coefficients $\mathbf{w}$.The input data $\mathbf{x}=(x_1,…,x_n)^T$ and the observed data$\mathbf{t}=(t_1,…,t_n)^T$,with a Gaussian noises variance $\sigma^2$ and a hyperparameter $\alpha$ representing the precision of the Gaussian prior over $\mathbf{w}$.<br>The joint distribution is given by the product of the prior $p(\mathbf{w})$ and $N$ conditional distributions $p(t_n|\mathbf{w})$ for $n=1,2,…,N$.<blockquote><p>$p(\mathbf{t,w})=p(\mathbf{w})\displaystyle \prod_{n=1}^{N}{p(t_n|\mathbf{w})}$</blockquote><h2 id=Appendix><a class=headerlink href=#Appendix title=Appendix></a>Appendix</h2><p>PDF: probability density function</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=url>Reading Paper:A Low-Complexity Massive MIMO Detection Based on Approximate Expectation Propagation(AEP or EPA in this paper)</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-17T09:05:55+08:00 title=创建于>2024-07-17</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-22T15:16:33+08:00 itemprop=dateModified title=更新于>2024-07-22</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>5 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Before-Beginning><a title="Before Beginning" class=headerlink href=#Before-Beginning></a>Before Beginning</h2><p>Now that we have some basic understanding of MIMO and relative mathematical knowledge,we will attempt to get the hang of this paper(whose knowledge still present a challenge to me)!<h3 id=A-Bit-of-Zero-Forcing-ZF><a title="A Bit of Zero Forcing(ZF)" class=headerlink href=#A-Bit-of-Zero-Forcing-ZF></a>A Bit of Zero Forcing(ZF)</h3><p>HFor a receiving antenna,if the channel matrix is $\mathbf{H}$,the sent message $\mathbf{s}$ and the received modulated message is $\mathbf{r}$ and denote the Gaussian noise as $\mathbf{n}$,then we denote the received message as:<blockquote><p>$\mathbf{r}=\mathbf{H\cdot s}+\mathbf{n}$</blockquote><p>The receiver don’t know about the sent messsage $\mathbf{s}$ so it decides to retrieve it from the received message $\mathbf{r}$.<br>Using MMSE,we can try to minimize$||\mathbf{r}-\mathbf{H\cdot s}||^2$<br>Obviously,we can get:<blockquote><p>$\hat{\mathbf{s}} = (\mathbf{H}^\mathbf{*})^{-1}\mathbf{H} \mathbf{r}$</blockquote><p>where the term before $\mathbf{r}$ is the pseudo-inverse of a matrix,$\mathbf{H^*}$ denotes the conjugate of $\mathbf{H}$.<br>The retrieved message,compared to the original $\mathbf{s}$,is:<blockquote><p>$\mathbf{\hat s}=\mathbf{s}+(\mathbf{n_m})$</blockquote><p>where $\mathbf{n_m}$ denotes the modified noise.<br>To eliminate the impact of possible influence of the modified noise,we add in a regularization factor $\delta \mathbf{I}$,so that:<blockquote><p>$$ \hat{\mathbf{s}} = (\mathbf{H}^\mathbf{*} + \delta \mathbf{I})^{-1} \mathbf{H} \mathbf{r} $$</blockquote><p>where:<br>if $\delta \to 0$,the channel matrix takes over so the $\hat {\mathbf s}$ will be just as the equation<br>if $\delta \to \infty$,the $\mathbf{I}$ takses over and in such cases,$\hat {\mathbf{s}}=\mathbf{H^*r}$<br>Completed!<h3 id=Neumann-Series><a title="Neumann Series" class=headerlink href=#Neumann-Series></a>Neumann Series</h3><h2 id=Introduction><a class=headerlink href=#Introduction title=Introduction></a>Introduction</h2><h3 id=Existing-Methods><a title="Existing Methods" class=headerlink href=#Existing-Methods></a>Existing Methods</h3><p>The loading factor $\alpha$ is defined as $\alpha=M/N$,with M transmitting and N receiving signals.<br>Existing MMSE and ZF methods exibit near -optimal performances when $\alpha <&LT1$,while carrying low spectral efficiency.<br>When $\alpha \approx 1$ the spectral efficiency can be attained while losing performance for linear detectors.<br>Later methods like BP and GTA still present not satisfying performance.<h3 id=Preliminary><a class=headerlink href=#Preliminary title=Preliminary></a>Preliminary</h3><h4 id=MIMO-System-Model><a title="MIMO System Model" class=headerlink href=#MIMO-System-Model></a>MIMO System Model</h4><p>In this paper,we use real-value system:<blockquote><p>$\mathbf{y}=\mathbf{Hx}+\mathbf{n}$</blockquote><p>where $\mathbf y\in \mathbb R^{2N}$,$\mathbf{H}\in \mathbb R^{2N\times 2M}$,$\mathbf{x}\in \Theta^{2M}$.$\mathbf n \sim \mathcal N(0,\sigma_n^2\mathbf I_{2N})$.<br>Assume a uniform prior distribution in this paper,meaning no tendency of a certain symbol.Expressed as $f(\mathbf x)\propto\prod_{i=1}^{2M}\mathbb I_{x_i\in \Theta}$,where $\mathbb I_{x_i\in \Theta}$ is the indicator duncition of the constraints of the value of $x_i\in \Theta$<br>If $x_i\in \Theta$,the function=1,else the function=0.<br>The posterior distribution of $\mathbf x$ has the following representation:<br>$p(\mathbf x|\mathbf y)\propto p(\mathbf y|\mathbf x)f(\mathbf x)\propto \mathcal N(\mathbf y :\mathbf{Hx},\sigma^2_n\mathbf I_{2N})\prod_{i=1}^{2M}{\mathbb I_{x_i\in \Theta}}$<h3 id=EP-MIMO-Detectors><a title="EP  MIMO Detectors" class=headerlink href=#EP-MIMO-Detectors></a>EP MIMO Detectors</h3><p>EP detector(EPD) enables a Gaussian approximation to be constructed for the posterior distribution of the transmitted symbols based on moment matching.<br>EP requires the explicit calculation of full matrix inversion,resulting in $\mathcal O(M^3)$ computation complexity.<br>The EP-NSA method exhibits great performance with $\alpha<&LT1$ but suffers performance degradation for larger $\alpha$.<br>The EP-SU method still costs around $\mathcal O(M^3)$.<br>The above methods involve circumventing <strong>matrix inversion</strong>!<br>Therefore,an EPA method is propopsed.Recovering EP performance for$\alpha&LT1$and approaches EP for $\alpha \geq 1$.<h3 id=Outline><a class=headerlink href=#Outline title=Outline></a>Outline</h3><p>The remainder of this paper is structured as below. The exact EP approach for massive MIMO detection is introduced in Section II. In Section III, detailed derivation, description and discussion of the proposed algorithm EPA are presented. Section IV shows an alternative derivation of EPA from the viewpoint of EC.Numericalsimulation results of the proposed algorithm and analysis of the computational costs are provided in Section V.Section VI concludes the paper.<h2 id=Simulation-Analysis><a title="Simulation Analysis" class=headerlink href=#Simulation-Analysis></a>Simulation Analysis</h2><p>Consider i.i.d.(independent and identically distributed)Rayleigh-fading channels with AWGN.<h2 id=Appendix><a class=headerlink href=#Appendix title=Appendix></a>Appendix</h2><p>ZF: zero forcing<br>CHEMP: channel-hardening exploiting message passing<br>GTA: Gaussian tree approximation<br>EP-NSA: EP with Neumann series approximation<br>EP-SU: EP-successive updating<br>EC: expectation consistency<br>i.i.d:independent and identically distributed<br>AWGN: additive white Gaussian noise</div><footer class=post-footer><div class=post-eof></div></footer></div></article><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><div class=post-block><link href=https://JohnnyZhu035.github.io/2024/07/15/compensation-KL-divergence/ itemprop=mainEntityOfPage><span hidden itemprop=author itemscope itemtype=http://schema.org/Person> <meta itemprop=name> <meta itemprop=description> <meta content=/images/avatar.png itemprop=image> </span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization> <meta content="JohnnyZhu's HomePage!" itemprop=name> </span><header class=post-header><h1 itemprop="name headline" class=post-title><a class=post-title-link href=/2024/07/15/compensation-KL-divergence/ itemprop=url>compensation:KL divergence</a></h1><div class=post-meta><span class=post-time> <span class=post-meta-item-icon> <i class="fa fa-calendar-o"></i> </span> <span class=post-meta-item-text>发表于</span> <time itemprop="dateCreated datePublished" datetime=2024-07-15T16:20:26+08:00 title=创建于>2024-07-15</time> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-calendar-check-o"></i> </span> <span class=post-meta-item-text>更新于:</span> <time datetime=2024-07-18T22:51:51+08:00 itemprop=dateModified title=更新于>2024-07-18</time> </span><span class=post-comments-count> <span class=post-meta-divider>|</span> <span class=post-meta-item-icon> <i class="fa fa-comment-o"></i> </span> <a href=/2024/07/15/compensation-KL-divergence/#comments itemprop=discussionUrl> <span class="post-comments-count disqus-comment-count" data-disqus-identifier=2024/07/15/compensation-KL-divergence/ itemprop=commentCount></span> </a> </span><div class=post-wordcount><span class=post-meta-item-icon> <i class="fa fa-clock-o"></i> </span><span title=阅读时长>3 mins.</span></div></div></header><div class=post-body itemprop=articleBody><h2 id=Reason><a class=headerlink href=#Reason title=Reason></a>Reason</h2><p>$\quad$In the Expectation Propagation(EP) algorithm,the KL (Kullback-Leibler) divergence plays an important role. Expectation Propagation is a technique used for approximate inference, commonly employed in probabilistic graphical models. In the Expectation Propagation algorithm, the KL divergence is used to measure the difference between two probability distributions in order to update variational parameters during the approximate inference process.<br>$\quad$Specifically, the Expectation Propagation algorithm approximates the <em>posterior</em> distribution by iteratively updating variational parameters. In each iteration, the KL divergence is used to quantify the <strong>discrepancy</strong> between the approximate posterior distribution and the true posterior distribution. By minimizing the KL divergence, the approximate posterior distribution can be brought closer to the true posterior distribution,enhancing the accuracy of the approximate inference.<h2 id=Definition><a class=headerlink href=#Definition title=Definition></a>Definition</h2><h3 id=What’s-Information-What’s-Entropy><a title="What’s Information?What’s Entropy?" class=headerlink href=#What’s-Information-What’s-Entropy></a>What’s Information?What’s Entropy?</h3><p>$\quad$We use information function $I(x)$ to measure the relationship between information and possibilities.For a certain event $x$,if the possibility that it happens is $P(x)$,then we define $I(x)$ as:<blockquote><p>$I(x)=-\log{(P(x))}$</blockquote><p>or<blockquote><p>$I(x)=\log{\frac{1}{(P(x))}}$</blockquote><p>Intuitively,the more possible,the less information.Match!<br>To sum up all information $I(x_i)$,we get the entropy!<blockquote><p>$H(X)=\displaystyle \sum_{i=1}^{n}{I(x_i)P(x_i)}$</blockquote><p>or<blockquote><p>$H(X)=\displaystyle -\sum_{i=1}^{n}{P(x_i)\log{(P(x_i))}}$</blockquote><p>where $X$ is a set with $n$ elements ${x_1,x_2,…,x_n}$,$x_i$ denotes a test.<h3 id=What’s-KL-Divergence><a title="What’s KL Divergence?" class=headerlink href=#What’s-KL-Divergence></a>What’s KL Divergence?</h3><p>KL(Kullback Leibler) divergence(or <em>relative entropy</em>) represents an asymmetric relationship between two probability distributions.<blockquote><p>$D_{KL}{(P||Q)}=\sum P(x)\log{}\frac{P(x)}{Q(x)}$ (in discrete scene)<br>$D_{KL}{(P||Q)}=\int P(x)\log{}\frac{P(x)}{Q(x)}dx$(in continuous scene)</blockquote><p>where P and Q are two distinct probability distributions on random variable $X$.<h3 id=What’s-Cross-Entropy><a title="What’s Cross Entropy?" class=headerlink href=#What’s-Cross-Entropy></a>What’s Cross Entropy?</h3><p>When we take a closer look at the KL divergence,we can break the $\log$ down into two pieces(take the discrete form as an example):<blockquote><p>$D_{KL}{(P||Q)}=\displaystyle \sum{P(x)\log(P(x))} -\sum{P(x)\log(Q(x))}$</blockquote><p>which can be further divided into:<blockquote><p>$-H(P(X))+[-\sum{P(x)\log{(Q(x))}}]$</blockquote><p>where we define the <em>cross entropy</em> of $P$ and $Q$ as:<blockquote><p>$H{(P,Q)}=\sum{-P(x)\log(Q(x))}$</blockquote><h2 id=Further-Application><a title="Further Application" class=headerlink href=#Further-Application></a>Further Application</h2><p><em><strong>To Be Continued</strong></em></div><footer class=post-footer><div class=post-eof></div></footer></div></article></section><nav class=pagination><span class="page-number current">1</span><a class=page-number href=/page/2/>2</a><a class="extend next" href=/page/2/ rel=next>></a></nav></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside class=sidebar id=sidebar><div class=sidebar-inner><section class="site-overview-wrap sidebar-panel sidebar-panel-active"><div class=site-overview><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img alt class=site-author-image itemprop=image src=/images/avatar.png><p class=site-author-name itemprop=name><p class="site-description motion-element" itemprop=description></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/archives/> <span class=site-state-item-count>15</span> <span class=site-state-item-name>日志</span> </a></div><div class="site-state-item site-state-tags"><a href=/tags/index.html> <span class=site-state-item-count>2</span> <span class=site-state-item-name>标签</span> </a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item> <a href=https://github.com/JohnnyZhu035 target=_blank title=GitHub> <i class="fa fa-fw fa-github"></i></a> </span><span class=links-of-author-item> <a href=213232918@seu.edu.cn target=_blank title=E-Mail> <i class="fa fa-fw fa-envelope"></i></a> </span></div></div></section><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div></aside></div></main><footer class=footer id=footer><div class=footer-inner><div class=copyright>© <span itemprop=copyrightYear>2024</span><span class=with-love> <i class="fa fa-user"></i> </span><span class=author itemprop=copyrightHolder>Johnny</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-area-chart"></i> </span><span title=站点总字数>NaNm</span><span class=post-meta-divider>|</span><span class=post-meta-item-icon> <i class="fa fa-coffee"></i> </span><span title=站点总阅读次数>NaN:aN</span></div><div class=theme-info>主题 — <a class=theme-link href=https://github.com/theme-next/hexo-theme-next target=_blank>NexT.Pisces</a> v6.0.0</div><span id=timeDate>载入天数...</span><span id=times>载入时分秒...</span><script>var now = new Date();
    function createtime() {
        var grt= new Date("7/10/2024 0:00:00"); //修改为你的网站开始运行的时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "这个网站已经上了这么多天班了 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);</script></div></footer></div><script>if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }</script><script src=/lib/jquery/index.js></script><script src=/lib/velocity/velocity.min.js></script><script src=/lib/velocity/velocity.ui.min.js></script><script src=/js/src/utils.js></script><script src=/js/src/motion.js></script><script src=/js/src/affix.js></script><script src=/js/src/schemes/pisces.js></script><script src=/js/src/bootstrap.js></script><script async id=dsq-count-scr src=https://johnnyzhuswebsite.disqus.com/count.js></script><script>// Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('-1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });</script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type=text/x-mathjax-config>
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script src=https://cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML></script>