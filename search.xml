<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>模电学习笔记1</title>
    <url>/2024/07/12/%E6%A8%A1%E7%94%B5%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01/</url>
    <content><![CDATA[<h1 id="本征半导体和杂质半导体"><a href="#本征半导体和杂质半导体" class="headerlink" title="本征半导体和杂质半导体"></a>本征半导体和杂质半导体</h1><h2 id="半导体"><a href="#半导体" class="headerlink" title="半导体"></a>半导体</h2><p>顾名思义，半导体就是指导电能力处于绝缘体和导体之间的物质，如硅，锗等。</p>
<h2 id="本征半导体"><a href="#本征半导体" class="headerlink" title="本征半导体"></a>本征半导体</h2><h3 id="本征半导体的定义"><a href="#本征半导体的定义" class="headerlink" title="本征半导体的定义"></a>本征半导体的定义</h3><p>本征半导体就是纯净的半导体，具有晶体结构。一般是四价元素，彼此靠共价键链接。<br>在绝对零度时，其中的价电子被束缚住，没有导电性能。<br>温度升高到常温下（300K）时，一些价电子得以脱离束缚，成为自由电子，相应的共价键上留下一个空位，称为空穴</p>
<h3 id="本征半导体的载流子"><a href="#本征半导体的载流子" class="headerlink" title="本征半导体的载流子"></a>本征半导体的载流子</h3><p>由定义可以发现，本征半导体的载流子分为两种：<br>1.自由电子(free electron)<br>2.空穴(mobile hole)</p>
<h3 id="本征半导体的载流子浓度"><a href="#本征半导体的载流子浓度" class="headerlink" title="本征半导体的载流子浓度"></a>本征半导体的载流子浓度</h3><p>公式：</p>
<blockquote>
<p>$n_i&#x3D;p_i&#x3D;K_1T^{3&#x2F;2}e^{-\frac{E_G}{2kT}}$</p>
</blockquote>
<p>其中：</p>
<blockquote>
<p>$n_i:\text{自由电子的浓度}$    $p_i:\text{空穴的浓度}$<br>$K_1:\text{系数（与材料有关）}$<br>$T:\text{绝对温度}$<br>$k:\text{玻尔兹曼常数}$<br>$E_G:\text{禁带宽度（价电子挣脱共价键所需能量）}$</p>
</blockquote>
<h3 id="本征半导体的导电能力"><a href="#本征半导体的导电能力" class="headerlink" title="本征半导体的导电能力"></a>本征半导体的导电能力</h3><p>有极限！半导体的导电能力有极限，如何改进？</p>
<h2 id="杂质半导体"><a href="#杂质半导体" class="headerlink" title="杂质半导体"></a>杂质半导体</h2><h3 id="杂质半导体的定义"><a href="#杂质半导体的定义" class="headerlink" title="杂质半导体的定义"></a>杂质半导体的定义</h3><p>注意到，半导体具有扩散特性，可将少量杂质元素掺入其中<br>如正四价元素中掺入五价（或者三价）元素，将少部分正四价元素取代<br>如正五价元素掺入之后，会出现很多自由电子<br>导电能力提高上百万倍！</p>
<h3 id="N型半导体"><a href="#N型半导体" class="headerlink" title="N型半导体"></a>N型半导体</h3><p>形成：掺入P（磷元素）<br>可出现两种载流子：自由电子（多数），空穴（少数）<br>因而自由电子是多子（多数载流子），空穴是少子（少数载流子）<br>由于带负电的自由电子占多数，其被称为 <em><strong>N（negative）型半导体</strong></em><br><strong>需要注意，每多一个自由电子，就会出现一个磷离子，因而整体依然呈现电中性</strong><br>温度敏感：由于温度影响的是本征激发，所以其对整体导电能力影响小，但是对少数载流子的影响依然大</p>
<h3 id="P型半导体"><a href="#P型半导体" class="headerlink" title="P型半导体"></a>P型半导体</h3><h4 id="P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体"><a href="#P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体" class="headerlink" title="P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体"></a>P有个洞！是空穴！所以多数载流子是空穴！其他同N型半导体</h4><h1 id="PN结"><a href="#PN结" class="headerlink" title="PN结"></a>PN结</h1><h2 id="PN结的定义"><a href="#PN结的定义" class="headerlink" title="PN结的定义"></a>PN结的定义</h2><p>顾名思义，PN结合在一起，就是PN结（在P，N型半导体的交界面形成），PN结具有单向导电性（可用于制作二极管）</p>
<h2 id="PN结的形成"><a href="#PN结的形成" class="headerlink" title="PN结的形成"></a>PN结的形成</h2><blockquote>
<p><img src="/images/PNJunction1.jpg" alt="图片" title="PN结示意图（1）"></p>
</blockquote>
<p>在PN半导体的交界面处，由于两种半导体中的载流子浓度差距很大，发生<strong>扩散运动</strong>，P区的空穴向N区扩散，N区的自由电子向P区扩散，如 <em>(a)<em>所示<br>在这其中P区出现负离子区，N区出现正离子区，这两个区域合起来叫做<strong>空间电荷区</strong>（或者<strong>耗尽层</strong>），形成电场。扩散运动强的时候（载流子浓度强），空间电荷区宽，其内部的电场强度加强。电场强度方向由N区指向P区，阻止了扩散运动的进行，在这之中P区的自由电子向N区流动，N区的空穴向P区流动（注意都是少子），这种运动就称为**漂移运动</em></em><br>这其中，当参与扩散运动的多子和参与漂移运动的少子数量达到动态平衡的时候，PN结形成。</p>
<h3 id="对称-不对称PN结"><a href="#对称-不对称PN结" class="headerlink" title="对称&#x2F;不对称PN结"></a>对称&#x2F;不对称PN结</h3><p>P区与N区杂质浓度相等时，负离子区和正离子区的宽度也相等，称为<strong>对称结</strong><br>浓度不相等时，浓度 <em>高</em>一侧的离子区宽度 <em>低</em>于浓度低一侧的离子区宽度，称为<strong>不对称PN结</strong></p>
<blockquote>
<p>理解：浓度高一侧的载流子扩散运动较强，将浓度低一侧的离子区拉宽</p>
</blockquote>
<p>两种PN结的外部特性相同</p>
<h2 id="PN结的单向导电性"><a href="#PN结的单向导电性" class="headerlink" title="PN结的单向导电性"></a>PN结的单向导电性</h2><h3 id="正向"><a href="#正向" class="headerlink" title="正向"></a>正向</h3><blockquote>
<p><img src="/images/PNJunction2.jpg" alt="图片" title="PN结示意图（2）"></p>
</blockquote>
<p>P接正极，N接负极时，称为PN结外加<strong>正向电压</strong>，外电场削弱内电场，使得扩散运动加剧，漂移运动减弱。显然，在外接正向电压达到一个临界数值之前，其电流不会有很大变化，但是一旦达到临界，电流就会急剧增大，形成<strong>正向电流</strong>，PN结<strong>导通</strong>，此时PN结的压降只有0.几V，因此需要串联限流电阻防止元器件损坏</p>
<blockquote>
<p>郑益慧老师：电源和二极管总要烧一个</p>
</blockquote>
<h3 id="反向"><a href="#反向" class="headerlink" title="反向"></a>反向</h3><blockquote>
<p><img src="/images/PNJunction3.jpg" alt="图片" title="PN结示意图（3）"></p>
</blockquote>
<p>加<strong>反向电压</strong>时，内电场被加强，空间电荷区（耗尽层）加宽，加剧了漂移运动，抑制了扩散运动，形成<strong>反向电流</strong>，由于漂移运动是少子在进行的，其数目极少，导致反向电流极小，一般可以忽略，认为其在加反向电压时处于<strong>截止</strong>状态。</p>
<h2 id="PN结的电流方程"><a href="#PN结的电流方程" class="headerlink" title="PN结的电流方程"></a>PN结的电流方程</h2><p>PN结所加端电压$u$与其电流$i$的关系为</p>
<blockquote>
<p>$i&#x3D;I_s(e^{\frac{u}{U_T}}-1)$</p>
</blockquote>
<p>常温下（$T&#x3D;300K$）时，$U_T\approx26mV$<br>称$U_T$为温度的电压当量</p>
]]></content>
  </entry>
  <entry>
    <title>Formula Practice(1)</title>
    <url>/2024/07/10/Formula-Practice-1/</url>
    <content><![CDATA[<h1 id="epsilon-delta-definition-of-a-limit"><a href="#epsilon-delta-definition-of-a-limit" class="headerlink" title="$\epsilon-\delta$ definition of a limit"></a>$\epsilon-\delta$ definition of a limit</h1><blockquote>
<p>Given a certain point $a \in R$ so that<br>for a certain function $f(x)$ exists the following  </p>
<blockquote>
<p>$\forall\epsilon &gt; 0$,<br>$\exists \delta &gt; 0$<br>if $|{x-a}| &lt; \delta$<br>so that $|{f(x)-A}| &lt; \delta$<br>then $\lim_{x \to a} &#x3D; A$</p>
</blockquote>
</blockquote>
<h1 id="离散信号（Discrete-Signal）的时域描述和分析"><a href="#离散信号（Discrete-Signal）的时域描述和分析" class="headerlink" title="离散信号（Discrete Signal）的时域描述和分析"></a>离散信号（Discrete Signal）的时域描述和分析</h1><h2 id="信号的采样和恢复"><a href="#信号的采样和恢复" class="headerlink" title="信号的采样和恢复"></a>信号的采样和恢复</h2><p>理想化的采样过程是一个将连续信号进行脉冲调制的过程，即</p>
<blockquote>
<p>$x_s(t) &#x3D; x(t)\delta_T(t) &#x3D; x(t) &#x3D; \sum_{n&#x3D;-\infty}^{\infty}{\delta(t-nT_s)} &#x3D; \sum_{-\infty}^{\infty}{x(nT_s)\delta(t-nT_s)}$<br>$x_s(t)$是经过采样处理后时间上离散化而幅值上仍然连续变化的信号，必须经过幅值上量化、编码处理等离散取值后才能成为数字信号。<br>一个连续信号离散化后，有两个问题需要讨论：（1）采样得到的信号$x_s(t)$在频域上有什么特性，它与原连续信号$x_(t)$的频域特性有什么联系？（2）连续信号采样后，它是否保留了原信号的全部信息，或者说，从采样的信号$x_s(t)$能否无失真地恢复原连续信号$x(t)$?<br>设连续信号$x(t)$的傅里叶变换为$X(\omega)$，采样后离散信号$x_s(t)$的傅里叶变换为$X_s(\omega)$，已知周期性冲激串$\delta_T(t)$的傅里叶变换为$P(\omega) &#x3D; \omega_s\sum_{n&#x3D;-\infty}^{\infty}{\delta(\omega-n\omega_s)}$，由傅里叶变换的频域卷积定理有<br>$X_s(\omega) &#x3D; \frac{1}{2\pi}X(\omega)*P(\omega)$<br>将$P(\omega)$代入上式，并按卷积运算的性质化简后得到抽样信号$x_s(t)$的傅里叶变换为<br>$X_s(\omega) &#x3D; \frac{1}{T_s}\sum_{n&#x3D;-\infty}^{\infty}{X(\omega-n\omega_s)}$<br>上式表明，一个连续信号经理想采样后频谱发生了两个变化：<br>1.频谱发生了周期延拓，即原连续信号的频谱$X(\omega)$延拓到以$\pm\omega_s$，$\pm2\omega_s$…为中心的频谱，其中$\omega_s$为采样角频率<br>1.频谱的幅度乘上了一个因子$\frac{1}{T_s}$，其中$T_s$为采样周期</p>
</blockquote>
<h2 id="时域采样定理"><a href="#时域采样定理" class="headerlink" title="时域采样定理"></a>时域采样定理</h2><p>对于频谱函数只在有限区间$(-\omega_m,\omega_m)$为有限值的频谱受限信号$x(t)$，为了将它的抽样信号$x_s(t)$恢复为原连续信号，只要对抽样信号施以截止频率为$w\geq\omega_m$的理想低通滤波，这时在频域上得到与$x(t)$的频谱$X(\omega)$完全一样的频谱（幅度的变化很容易实现）。对应地，在时域上也就完全恢复了原连续信号$x(t)$。从图中可以看出，上述连续信号恢复过程是在$\omega_s\geq\omega_m$的前提下实现的，也即采样频率至少为原连续信号所含最高频率成分的2倍时实现的。这时，就能够无失真地从抽样信号中恢复原连续信号，或者说，采样过程完全保留了原信号的全部信息。<br>当$\omega_s&lt;2\omega_m$时，在频域就会出现频谱混叠现象。施以理想低通滤波后不能得到与$X(\omega)$完全一样的频谱。可以想象，在时域也就不能无失真地恢复原连续信号$x(t)$由此，得出关于采样频率如何取的结论，这就是著名的时域采样定理（香农定理）：</p>
<p>对于频谱受限的信号$x(t)$，如果其最高频率分量为$\omega_m$，为了保留原信号的全部信息，或能无失真地恢复原信号，在通过采样得到离散信号时，其采样频率应满足$\omega_s\geq2\omega_m$。通常把最低允许的采样频率$\omega_s&#x3D;2\omega_m$称为奈奎斯特（Nyquist）频率。</p>
<p>为了从抽样信号$x_s(t)$中恢复原信号$x(t)$，可将抽样信号的频谱$X_s(\omega)$乘上幅度为$T_s$的矩形窗信号</p>
<blockquote>
<p>$G(\omega)&#x3D;\begin{cases}T_s&amp;|\omega|\leq\frac{\omega_s}{2} \\ 0&amp;|\omega|&gt;\frac{\omega_s}{2}\end{cases}$</p>
</blockquote>
<p>它将原信号的频谱$X(\omega)$从$X_s(w)$中完整的提取出来，即</p>
<blockquote>
<p>$X(\omega)&#x3D;X_s(\omega)G(\omega)$</p>
</blockquote>
<p>由傅里叶时域卷积性质有：</p>
<blockquote>
<p>$x(t)&#x3D;x_s(t)*g(t)$</p>
</blockquote>
<p>而</p>
<blockquote>
<p>$g(t)&#x3D;Sa(\frac{\omega_s}{2}t)$</p>
</blockquote>
<p>所以</p>
<blockquote>
<p>$x(t)&#x3D;\sum_{n&#x3D;-\infty}^{\infty}{x(nT_s)\delta(t-nT_s)}*Sa(\frac{\omega}{2}t)&#x3D;\sum_{n&#x3D;-\infty}^{\infty}{x(nT_s)Sa(\frac{\omega_s}{2}(t-nT_s))}$</p>
</blockquote>
<p>如果恰好$\omega_m&#x3D;\frac{1}{2}\omega_s$，则</p>
<blockquote>
<p>$x(t)&#x3D;\sum_{-\infty}^{\infty}{x(nT_s)Sa[\omega_m(t-nT_s)]}&#x3D;\sum_{n&#x3D;-\infty}^{\infty}{x(nT_s)\frac{sin\omega_m(t-nT_s)}{\omega_m(t-nT_s)}}$</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>Reading Paper:factor graphs and the sum product algorithm</title>
    <url>/2024/07/14/Reading-Paper-factor-graphs-and-the-sum-product-algorithm/</url>
    <content><![CDATA[<h2 id="Factor-Graph-因子图"><a href="#Factor-Graph-因子图" class="headerlink" title="Factor Graph(因子图)"></a>Factor Graph(因子图)</h2><h3 id="composition"><a href="#composition" class="headerlink" title="composition"></a>composition</h3><p>Factor graphs are composed of two parts:<em>(random)variables and factors</em><br>e.g. a funtion imply(A,B) has the meaning of if A,then B<br>here represents two variables: A:Dave is drunk B:Dave can’t drive<br>as is obvious,if a person is drunk,then he(she) can’t drive<br>hence a simple$f(drunk,can’t\space drive)$represents the meaning.</p>
<h3 id="weight-of-functions–probability"><a href="#weight-of-functions–probability" class="headerlink" title="weight of functions–probability"></a>weight of functions–probability</h3><p><em>To define the probability of a possible world,we define through factor functions</em><br><strong>weights</strong>are given to different factor funtions,to express the relative influence of each factor on the probability.Factors can be given larger weight to imply a higher impact on probability.</p>
<blockquote>
<p><strong>Amazingly,when I refer to a post at <a href="http://deepdive.stanford.edu/assets/factor_graph.pdf">http://deepdive.stanford.edu/assets/factor_graph.pdf</a> to look up for definitions,the post contains the link of the <em>Factor Graphs and the Sum-Product Algorithm</em> ,which really surprises me</strong></p>
</blockquote>
<blockquote>
<p>the <em>Factor Graphs and the Sum-Product Algorithm</em> mentioned the factor graph has the potential to unify modeling and signal processing tasks that are often treated separately in current systems,then if now,some 20 years have passed,any development based on it?</p>
</blockquote>
<h3 id="history–why-factor-graph"><a href="#history–why-factor-graph" class="headerlink" title="history–why factor graph"></a>history–<em>why factor graph</em></h3><p>factor graphs are a generalization of the “Tanner graphs”named after Tanner.He introduced the bipartite graphs(dividing group of nodes into two parts connected by edges)to generalize family of codes performing low-density-parity-check(LDPC) and described the sum-product algorithm.Tanner and Wiberg respectively introduced the “visible” and “hidden”variables.However,by applying these models to functions,the factor graph becomes a higher abstraction.Viewing from the perspective of factor graph,the Tanner graph represents a particular factorization of the code.</p>
<h4 id="marginal-product-of-functions-not-in-this-algorithm-unfortunately"><a href="#marginal-product-of-functions-not-in-this-algorithm-unfortunately" class="headerlink" title="marginal product of functions(not in this algorithm,unfortunately)"></a><em>marginal product of functions</em>(not in this algorithm,unfortunately)</h4><p>The marginal product stems from the total product,which describes that for a set of variables(e.g.$x_1,x_2,x_3$),the overall output is $F(x_1,x_2,x_3)$.Set the other inputs fixed(say fix $x_2,x_3$ are fixed to $a,b$) and simply vary one input(say vary $x_1$),then the output becomes $F(x_1,a,b)$,which,in other words,can be described as the total product function $TP(x_1)$.<br>The marginal product function describes the slope of the total product funtion.If the total product function is differentiable,then the marginal product can be described as$MP(x_1)&#x3D;\frac{dTP(x_1)}{dx_1}$<br>btw,the average product of a function is described as $AP(x_1)&#x3D;\frac{TP(x_1)}{x_1}$</p>
<h2 id="Marginal-Functions-Factor-Graphs-and-the-Sum-Product-Algorithm"><a href="#Marginal-Functions-Factor-Graphs-and-the-Sum-Product-Algorithm" class="headerlink" title="Marginal Functions,Factor Graphs and the Sum-Product Algorithm"></a>Marginal Functions,Factor Graphs and the Sum-Product Algorithm</h2><p>Let n variables be $x_1,x_2,…,x_n$,each $x_i \in A_i$.<br>Let $g(x_1,…,x_n)$be an R-valued function of variables $x_1,x_2,,…,x_n$<br>so that the function $g$ is with domain $S&#x3D;A_1 \times A_2 \times … \times A_n$ and codomain $R$.<br>$S$ is called the <em>configuration space</em>for the given variables.Each element is a particular <em>configuration</em> of the variables<br>associated with every function $g(x1,…,x_n)$are $n$ marginal functions $g_i(x_i)$<br><strong>And for each $a\in A_i$,the value of $g_i(a)$ is obtained by summing the value of $g(x_1,…,x_n)$over all configurations of the variables that have $x_i&#x3D;a$</strong><br>The paper proposes a nontraditional notation–the”not-sum” or <em>summary</em>.For example,if $g$ is a function of variables$x_1,x_2,x_3$,then the <em>summary</em> for $x_2$ can be denoted by:</p>
<blockquote>
<p>$g_2(x_2)&#x3D;\sum_{\sim{x_2}}{g(x_1,x_2,x_3)}&#x3D;\sum_{x_1\in A_1}{\sum_{x_3\in A_3}{g(x_1,x_2,x_3)}}$</p>
</blockquote>
<p>in this sense:</p>
<blockquote>
<p>$g_i(x_i)&#x3D;\sum_{\sim{x_i}}{g(x_1,…,x_n)}$</p>
</blockquote>
<p>that means the $i$th marginal function associated with $g$ is the <em>summary</em> for $x_i$ of $g$<br>In this paper,another topic is about transforming global functions into the product of several global functions.<br>Let $g(x_1,…,x_n)$ factor into the product of several <em>local functions</em> $f_i$,each having some subset of ${x_1,…,x_n}$ as arguments (e.g. $J&#x3D;{1,2,3,4,5}, X_1&#x3D;{x_1,x_3,x_5},X_2&#x3D;{x_2,x_3}$).</p>
<blockquote>
<p>$g(x_1,…,x_n)&#x3D;\prod_{j\in J}{f_j(X_j)}$</p>
</blockquote>
<p>Followed by several examples of factor graphs<br>A key is that <em><strong>a cycle-free factor graph not only encodes in its structure but also encodes arithmetic expressions by which the marginal functions associated with the global functions may be computed.</strong></em></p>
<h2 id="Core-of-the-algorithm"><a href="#Core-of-the-algorithm" class="headerlink" title="Core of the algorithm"></a>Core of the algorithm</h2><h3 id="Sum-And-Product"><a href="#Sum-And-Product" class="headerlink" title="Sum And Product"></a>Sum And Product</h3><p>As its name suggests,the algorithm can be derived into two parts:the sum part and the product part,which cna be transformed into each other with ease.Noticably,the graphical demonstration of it involves the factor graph.</p>
<h3 id="Converted-factor-graph–expression-trees"><a href="#Converted-factor-graph–expression-trees" class="headerlink" title="Converted factor graph–expression trees"></a>Converted factor graph–expression trees</h3><p>expression trees are a kind of tree that contain arithmetic operators($+,\times$…etc.)in internal vertices and variables or constants in leaf vertices.<br>An extended version of the expression tree is proposed,using <em>functions</em> besides variables and constants.<br>replace variable node with a product operator.<br>replace factor node with a “form product” and multiply by”$f$” operator.</p>
<blockquote>
<p>my own interpretation:<br><em>both</em><br><strong>bottom-up approach</strong><br>$f$ is the local function that operates<br>$x$ works as variables</p>
</blockquote>
<blockquote>
<p><em>In expression tree</em><br>the tree itself expresses the algorithm<br>$+ \text{,and}\times$ work as operators to combine these nodes together</p>
</blockquote>
<blockquote>
<p><em>In rooted tree</em><br>expresses the process and relationship between nodes<br>factor nodes and variable nodes come in an alternating form.</p>
</blockquote>
<h3 id="Sum-Product-Updating-Rule"><a href="#Sum-Product-Updating-Rule" class="headerlink" title="Sum Product Updating Rule"></a>Sum Product Updating Rule</h3><h4 id="update"><a href="#update" class="headerlink" title="update"></a>update</h4><p>a node receives all messages from nodes adjacent to it except for the node to send message<br>In this approach,all adjacent nodes of node $v$ are denoted as $n(v)$.<br>Let the message be $\mu_{x\to f}{(x)}$ ,denoting message sent from node $f$ to node $x$,and $\mu_{f\to x}{(x)}$ be message sent from node $x$ to node $f$.<br>The rules are as follows:<br><em>variable to local function</em><br>$\mu_{x\to f}{(x)}&#x3D;\prod_{h\in n(x) \backslash {f}}{\mu_{h\to x}{(x)}}$<br>simply speaking:to operate product on all messages from other nodes and send them to the descending factor node $f$.<br><em>local function to variable</em><br>$\mu_{f\to x}{(x)}&#x3D;\sum_{\sim{x}}{(f(X)\prod_{y\in n(f)\backslash{x}}{\mu_{y\to f}{(y)}})}$<br>simply put:to operate product of all messages received and times the local function,sum up the results with index $\sim{x}$<br>$X&#x3D;n(f)$ is the set of arguments of $f$</p>
<h4 id="end-termination"><a href="#end-termination" class="headerlink" title="end(termination)"></a>end(termination)</h4><p>Representing the arithmetic form with factor graph,we can easily see that for a variable node $x_i$,its”summary”$g_i(x_i)$ can be described as the product of all messages sent from all edges to the specific node.<br>For example,if a variable node $x_1$ is connected to factor nodes $f_1$ and $f_3$,then the marginal function of $x_1$ can be denoted as</p>
<blockquote>
<p>$g_1(x_1)&#x3D;\mu_{f_1\to x_1}{(x_1)}\mu_{f_3\to x_1}{(x_1)}$</p>
</blockquote>
]]></content>
      <tags>
        <tag>Reading-Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Reading Paper:Belief-selective Propagation Detection for MIMO Systems</title>
    <url>/2024/07/14/Reading-Paper-Belief-selective-Propagation-Detection-for-MIMO-systems/</url>
    <content><![CDATA[<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>$\quad$For a first year student who have been arranged to not take courses like <em>probability theory</em>,the famed Bayesian things are not familiar to me at all.So I prepare some prerequisite knowledge here!</p>
<h3 id="Bayes’-theorem"><a href="#Bayes’-theorem" class="headerlink" title="Bayes’ theorem"></a>Bayes’ theorem</h3><p>$\quad$In this theorem,we define two events $A$ and $B$,each having $P(A),P(B)$ to denote their possibilities of happening.The Bayes’ theorem considers the influence on the possibilities of each other.We denote event that if B happens,A also happens using $A|B$.<br>The Bayes’ theorem tells us that</p>
<blockquote>
<p>$P(A|B)&#x3D;\frac{P(B|A)P(A)}{P(B)}$</p>
</blockquote>
<p>we give each possibility in this formula<br>a name:</p>
<blockquote>
<p>$P(A)$ is $A$’s prior probability(which means it does not take any related event into account,only considering the general case)<br>$P(A|B)$ is $A$’s posterior probability(which means that after B happens,the possibility that A happens should be refreshed)</p>
</blockquote>
<h3 id="Maximum-Likelihood-Estimate-MLE"><a href="#Maximum-Likelihood-Estimate-MLE" class="headerlink" title="Maximum Likelihood Estimate(MLE)"></a>Maximum Likelihood Estimate(MLE)</h3><p>$\quad$For a random test(like flipping coins)<br>define the possibility of event <em>Heads</em> $\theta$,then event <em>Tails</em> is defined as $(1-\theta)$.in one test,<em>Heads</em> happen $a$ times and <em>Tail</em> happen $b$ times.We define the possibilty that this test happens as $N$.Define the possibility $P(N|\theta)$as the <em>likelihood function</em> $L(\theta)$:</p>
<blockquote>
<p>$L(\theta)&#x3D;\theta^a*(1-\theta)^b$</p>
</blockquote>
<p>where we need to find the most possible point to let the function have its extreme point.Take the logarithm on both sides of the function equation:</p>
<blockquote>
<p>$\log{(L(\theta))}&#x3D;a\log{(\theta)}+b\log{(1-\theta)}$</p>
</blockquote>
<p>take the derivative of it,we can easily get the extreme point!</p>
<h3 id="Maximum-a-Posterioris-Estimate-MAPE"><a href="#Maximum-a-Posterioris-Estimate-MAPE" class="headerlink" title="Maximum a Posterioris Estimate(MAPE)"></a>Maximum <em>a Posterioris</em> Estimate(MAPE)</h3><p>$\quad$With the two parts of knowledge in mind,we are well ready to get our hands on Maximum <em>a posterioris</em>!<br>$\quad$As its name suggests,<em>a posterioris</em> means after.Then  estimating based on prior tests is the core of this method.<br>$\quad$We define a set of tests $S$,and the desired possibility of the event $\theta$ is happening is defined as</p>
<blockquote>
<p>$P(\theta|S)$</p>
</blockquote>
<p>$\quad$According to the Bayes’ theorem,the possibility above can be described as</p>
<blockquote>
<p>$P(\theta|S)&#x3D;\frac{P(S|\theta)P(\theta)}{P(S)}$</p>
</blockquote>
<p>$\quad$Obviously,the $P(S|\theta)$ refers  to the <em>most likelihood</em> function $L(\theta)$.Easily,we can calculate the MAP possibility.</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>simplifying calculations will cost performance quality?<br>utilizing <em>trusted</em> incoming messages with <em>a priori</em> messages for updates.<br>proposed two strategies:</p>
<blockquote>
<p>symbol-based truncation(ST)(基于特征的去杂)<br>edge-based simplification(ES)(基于边缘的简化)<br>conveniently tuning parameters can make great trade-off between performance and complexity</p>
</blockquote>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>$\quad$MIMO enables high spectrum-efficiency and energy-efficiency.<br>$\quad$Current <em>maximum a posteriori</em> (MAP) detection(最大后验检测) or the <em>maximum likelihood</em> detection(极大似然检测) can achieve minimum error probability while costing exponential complexity with modulation order and MIMO scale.<br>$\quad$Sphere decoder(SD) restricts candidate symbols to ones within a sphere (depth first) or a list  of size $K$ (width first).It performs well in small-size MIMO, while for large-scale MIMO it suffers cubic($\mathcal{O}^3$) time complexity.<br>$\quad$For large-scale MIMO,linear detectors like <em>zero-forcing</em>(ZF) detector or <em>linear minimum mean square error</em>(LMMSE) detector are often applied,performing way worse than the previous methods.<br>$\quad$Another problem is the matrix inversion,bringing excessive complexity.Even new methods like the <em>Neumann series approximation</em> can’t fully exploit the MIMO benefits.<br>$\quad$<em>Finally</em>,the Belief Propagation(BP)detector based on the Bayes’ tule is proposed.It can delever near-optimal performance and holds a soft-input soft-output character.And it can be easily applied to various hardware and scaled for applications.<br>$\quad$The BP method achieves near-optimal when in small-to medium MIMO but costs exponentially increasing complexity.<br>$\quad$The work in this paper focuses on both reducing the complexity and maintaining performance.</p>
<!--So far, the design of efficient MIMO BP detectors remains challenging. First, the existing
 BP detectors suffer an aggravated performance error floor in relatively high signal-to-noise ratio
 (SNR) region, due in part to the well-known impact of the inherent loopy structure for the full
connected FG model [23], in part to the approximations such as GAI. Second, the complexity of
 existing BP detectors is still high and not flexible enough for various applications. These multiple
 challenges motivate us(us?) to think of an approach which can reduce the detection complexity while
 mitigating the error floor towards better performance.-->
<p> $\quad$This aim is mainly achieved by introducing the Belief-selective Propagation(BsP),utilizing <em>a priori</em> (AP) probabilities to update incoming messages to output messages.This approach avoids propagating “low belief” messages.</p>
<h2 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h2><p>$N_t,N_r$ denote FN number and SN number(implying $N_r$ messages).</p>
<h3 id="System-Model"><a href="#System-Model" class="headerlink" title="System Model"></a>System Model</h3><p>QAM:Quadrature Amplitude Modulation<br>$s&#x3D;[s_1,s_2,…,s_N{_t}]^T$<br>The flat-fading complex MIMO channel matrix $\mathbf {H}$ is a $N_r\times N_t$ matrix:</p>
<blockquote>
<p>$\mathbf {H}&#x3D;[\mathbf {h_1}^{T},\mathbf {h_2}^{T},…\mathbf {h_{N_r}^{T}}]^T$</p>
</blockquote>
<p>where each component $\mathbf {h_i}$ is a complex channel coefficient.<br> <!--is a complex channel coefficient following the zero-mean and unit-variance Gaussian distribution
 Why this is less unlikely to happen in the future communication?--></p>
<h3 id="BP-Detection-With-the-FG-Model"><a href="#BP-Detection-With-the-FG-Model" class="headerlink" title="BP Detection With the FG Model"></a>BP Detection With the FG Model</h3><p>$\quad$In a <em>factor graph</em>(FG) model,there are two types of nodes,namely <em>factor node</em>(FN) and <em>symbol node</em>(SN),denoted in graphs as:<br>$\begin{cases}f_i \Leftrightarrow FN, &amp; i\in {1,2,…,N_r} \ S_j\Leftrightarrow SN, &amp; j\in {1,2,…,N_t}\end{cases}$<br>$\quad$In both the factor graph and the tanner graph,we use the channel matrix $\mathbf {H}$ to denote elements. To be exact,each $f_i$ corresponds to the $i$-th row of the channel matrix,and each $S_j$ relates to the $j$-th column of the channel matrix. Every pair<br>of $f_i$ and $S_j$ are connected with an edge corresponding to the channel coefficient $h_{ij}$.<br>$\quad$In this article ,message $\beta_{ij}$ will be delivered from $f_i$ to $S_j$,while the message $\alpha$ will be computed and transferred in a reverse direction,meaning $\alpha_{ji}$ will be message sent from $S_j$ to $f_i$.</p>
<h2 id="The-Proposed-BsP-Detection"><a href="#The-Proposed-BsP-Detection" class="headerlink" title="The Proposed BsP Detection"></a>The Proposed BsP Detection</h2><p>$\quad$The constellation cardinality($|(\mathcal A)|$) of the MIMO systems and the degree<!--?--> of the function nodes(FNs) contribute greatly to the exponential computational complexity.<br>$\quad$However symbol vectors with low reliability contribute limitly to the message $\beta_{ij}$,encouraging us to squeeze the search space by removing them.</p>
<!-- 1)STstrategy to reduce thecardinalityof the
 transmittedsymbol
  2)ESstrategy tosimplified theconnected
 edgesof theFNs-->
<p><strong>$\quad$The two strategies can be concluded in a simple metaphor.Here comes a bunch of people $\alpha_{ji}$,with $j\in \mathbf{k}$,each with a number of messages.Based on the credibility of each person,you select $d_f$ most credible people to receive message,being the ES strategy.For each independent person, you select $d_m$ most credible messages to hear,being the ST strategy.</strong></p>
<h3 id="Symbol-Based-Truncation-Strategy"><a href="#Symbol-Based-Truncation-Strategy" class="headerlink" title="Symbol-Based Truncation Strategy"></a>Symbol-Based Truncation Strategy</h3><p><strong>Here,this ST strategy can be simply described in a sentence:Man</strong><br>$\quad$Incoming $\alpha_{ji}{}$ is truncated into $\alpha_{ji}^{t}$ by eliminating the <em>log-likelihood ratio</em>(LLR) with relatively small value.<br>$\quad$Formula unheard of to compute message $\beta_{ij}$.With $\mu_k$ and $\mu_1$,I think this in part looks like the <strong>LLR</strong>?<br>$\quad$Core is to eliminate low credibility incoming data $\alpha$ into $\alpha^t$(truncated).This procedure is basesd on the LLR.<br>$\quad$In this paper,this method(ST) successfully made it to reduce the number of possible choices of transmitted symbols to:</p>
<blockquote>
<p>$\psi_{ST}&#x3D;|\mathcal A|\times |\mathcal B(d_m)|$</p>
</blockquote>
<p>where $\mathcal B(d_m)$ denotes the configuration set of possible choices of $\mathbf{s_k}_{\backslash j}$,with $|\mathcal B(d_m)|&#x3D;(d_m)^{(N_t-1)}$<br>Because $d_m &lt;&lt; |\mathcal A|$,the ST strategy reduces the computaional complexity.</p>
<h3 id="Edge-Based-Simplification-Strategy"><a href="#Edge-Based-Simplification-Strategy" class="headerlink" title="Edge-Based Simplification Strategy"></a>Edge-Based Simplification Strategy</h3><p>Apart from the ST strategy,the ES strategy further reduces the computational complexity.<br>The core if it is to directly decide some of the transmitted symbol $\mathbf {s_{k}}_{\backslash j}$.<br>By choosing the most reliable message (judging by LLR),we can denote the new configuration set as$\mathcal B(|\mathcal A|,d_f)$:</p>
<blockquote>
<p>$\mathcal  B(|\mathcal A|,d_f)&#x3D;{ {\mathbf{s_k}_ {\backslash j} }&#x3D;[\mathbf { {s_k} }<em>{\backslash j,d_f};\mathbf {\bar{s_k}</em>{\backslash j,d_f}^T]} }$</p>
</blockquote>
<p>In this sense,the cardinality of $\mathcal B(|\mathcal A|,d_f)$ is</p>
<blockquote>
<p>$|\mathcal B(|\mathcal A|,d_f)|&#x3D;\binom{N_t-1} {d_f-1}|\mathcal A|^{d_f-1}$</p>
</blockquote>
<p>and the number of possible choices are reduced by the ES strategy to:</p>
<blockquote>
<p>$\psi_{ES}&#x3D;|\mathcal A|\times \binom{N_t-1} {d_f-1}|\mathcal A|^{d_f-1}$</p>
</blockquote>
<h3 id="Initialization-with-PP-Information"><a href="#Initialization-with-PP-Information" class="headerlink" title="Initialization with PP Information"></a>Initialization with PP Information</h3><p>Core algorithm:LMMSE(Linear Minimum Mean Square Estimate).</p>
<blockquote>
<p>$\hat{\mathbf s}_{\text{MMSE} }&#x3D;(\mathbf{H}^H\mathbf H+\sigma^2\mathbf{I})^{-1}\mathbf{H}^H\mathbf{y}$.</p>
</blockquote>
<p>Where we denote:</p>
<blockquote>
<p>$$\mathbf{K}&#x3D;(\mathbf H^H\mathbf H+\sigma^2\mathbf{I})^{-1}$$</p>
</blockquote>
<p>With such,<em>a priori</em> probabilities of transmitted symbols are computed by</p>
<blockquote>
<p>$p_j(\mu_k) &#x3D; \exp {-\frac{||\mu_k - \hat{\mathbf{s}}{\text{MMSE}}||^2}{2\sigma{\text{MMSE}_j}^2} }$</p>
</blockquote>
<h3 id="The-Proposed-BsP-Detector"><a href="#The-Proposed-BsP-Detector" class="headerlink" title="The Proposed BsP Detector"></a>The Proposed BsP Detector</h3><h4 id="Algorithm-of-the-BsP-Detection"><a href="#Algorithm-of-the-BsP-Detection" class="headerlink" title="Algorithm of the BsP Detection"></a>Algorithm of the BsP Detection</h4><p>This is a rather interesting algorithm!<br>Input:received signal $\mathbf y$ and channel matrix $\mathbf H$<br>Output:soft information of coded bits $\mathbf r$</p>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>PP: pseudo priori<br>(|$\mathcal A$|): constellation cardinality<br>LLR: log-likelihood ratio<br>AMI: average mutual information<br>BER: bit error rate or block error rate(误码率和误信率)</p>
]]></content>
      <tags>
        <tag>Reading-Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>compensation:KL divergence</title>
    <url>/2024/07/15/compensation-KL-divergence/</url>
    <content><![CDATA[<h2 id="Reason"><a href="#Reason" class="headerlink" title="Reason"></a>Reason</h2><p>$\quad$In the Expectation Propagation(EP) algorithm,the KL (Kullback-Leibler) divergence plays an important role. Expectation Propagation is a technique used for approximate inference, commonly employed in probabilistic graphical models. In the Expectation Propagation algorithm, the KL divergence is used to measure the difference between two probability distributions in order to update variational parameters during the approximate inference process.<br>$\quad$Specifically, the Expectation Propagation algorithm approximates the <em>posterior</em> distribution by iteratively updating variational parameters. In each iteration, the KL divergence is used to quantify the <strong>discrepancy</strong> between the approximate posterior distribution and the true posterior distribution. By minimizing the KL divergence, the approximate posterior distribution can be brought closer to the true posterior distribution,enhancing the accuracy of the approximate inference.</p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><h3 id="What’s-Information-What’s-Entropy"><a href="#What’s-Information-What’s-Entropy" class="headerlink" title="What’s Information?What’s Entropy?"></a>What’s Information?What’s Entropy?</h3><p>$\quad$We use information function $I(x)$ to measure the relationship between information and possibilities.For a certain event $x$,if the possibility that it happens is $P(x)$,then we define $I(x)$ as:</p>
<blockquote>
<p>$I(x)&#x3D;-\log{(P(x))}$</p>
</blockquote>
<p>or</p>
<blockquote>
<p>$I(x)&#x3D;\log{\frac{1}{(P(x))}}$</p>
</blockquote>
<p>Intuitively,the more possible,the less information.Match!<br>To sum up all information $I(x_i)$,we get the entropy!</p>
<blockquote>
<p>$H(X)&#x3D;\displaystyle \sum_{i&#x3D;1}^{n}{I(x_i)P(x_i)}$</p>
</blockquote>
<p>or</p>
<blockquote>
<p>$H(X)&#x3D;\displaystyle -\sum_{i&#x3D;1}^{n}{P(x_i)\log{(P(x_i))}}$</p>
</blockquote>
<p>where $X$ is a set with $n$ elements ${x_1,x_2,…,x_n}$,$x_i$ denotes a test.</p>
<h3 id="What’s-KL-Divergence"><a href="#What’s-KL-Divergence" class="headerlink" title="What’s KL Divergence?"></a>What’s KL Divergence?</h3><p>KL(Kullback Leibler) divergence(or <em>relative entropy</em>) represents an asymmetric relationship between two probability distributions.</p>
<blockquote>
<p>$D_{KL}{(P||Q)}&#x3D;\sum P(x)\log{}\frac{P(x)}{Q(x)}$ (in discrete scene)<br>$D_{KL}{(P||Q)}&#x3D;\int P(x)\log{}\frac{P(x)}{Q(x)}dx$(in continuous scene)</p>
</blockquote>
<p>where P and Q are two distinct probability distributions on random variable $X$.</p>
<h3 id="What’s-Cross-Entropy"><a href="#What’s-Cross-Entropy" class="headerlink" title="What’s Cross Entropy?"></a>What’s Cross Entropy?</h3><p>When we take a closer look at the KL divergence,we can break the $\log$ down into two pieces(take the discrete form as an example):</p>
<blockquote>
<p>$D_{KL}{(P||Q)}&#x3D;\displaystyle \sum{P(x)\log(P(x))} -\sum{P(x)\log(Q(x))}$</p>
</blockquote>
<p>which can be further divided into:</p>
<blockquote>
<p>$-H(P(X))+[-\sum{P(x)\log{(Q(x))}}]$</p>
</blockquote>
<p>where we define the <em>cross entropy</em> of $P$ and $Q$ as:</p>
<blockquote>
<p>$H{(P,Q)}&#x3D;\sum{-P(x)\log(Q(x))}$</p>
</blockquote>
<h2 id="Further-Application"><a href="#Further-Application" class="headerlink" title="Further Application"></a>Further Application</h2><p><em><strong>To Be Continued</strong></em></p>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Reading Paper:A Low-Complexity Massive MIMO Detection Based on Approximate Expectation Propagation(AEP or EPA in this paper)</title>
    <url>/2024/07/17/Reading-Paper-A-Low-Complexity-Massive-MIMO-Detection-Based-on-Approximate-Expectation-Propagation-AEP-or-EPA-in-this-paper/</url>
    <content><![CDATA[<h2 id="Before-Beginning"><a href="#Before-Beginning" class="headerlink" title="Before Beginning"></a>Before Beginning</h2><p>Now that we have some basic understanding of MIMO and relative mathematical knowledge,we will attempt to get the hang of this paper(whose knowledge still present a challenge to me)!</p>
<h3 id="A-Bit-of-Zero-Forcing-ZF"><a href="#A-Bit-of-Zero-Forcing-ZF" class="headerlink" title="A Bit of Zero Forcing(ZF)"></a>A Bit of Zero Forcing(ZF)</h3><p>HFor a receiving antenna,if the channel matrix is $\mathbf{H}$,the sent message $\mathbf{s}$ and the received modulated message is $\mathbf{r}$ and denote the Gaussian noise as $\mathbf{n}$,then we denote the received message as:</p>
<blockquote>
<p>$\mathbf{r}&#x3D;\mathbf{H\cdot s}+\mathbf{n}$</p>
</blockquote>
<p>The receiver don’t know about the sent messsage $\mathbf{s}$ so it decides to retrieve it from the received message $\mathbf{r}$.<br>Using MMSE,we can try to minimize$||\mathbf{r}-\mathbf{H\cdot s}||^2$<br>Obviously,we can get:</p>
<blockquote>
<p>$\hat{\mathbf{s}} &#x3D; (\mathbf{H}^\mathbf{*})^{-1}\mathbf{H} \mathbf{r}$</p>
</blockquote>
<p>where the term before $\mathbf{r}$ is the pseudo-inverse of a matrix,$\mathbf{H^*}$ denotes the conjugate of $\mathbf{H}$.<br>The retrieved message,compared to the original $\mathbf{s}$,is:</p>
<blockquote>
<p>$\mathbf{\hat s}&#x3D;\mathbf{s}+(\mathbf{n_m})$</p>
</blockquote>
<p>where $\mathbf{n_m}$ denotes the modified noise.<br>To eliminate the impact of possible influence of the modified noise,we add in a regularization factor $\delta \mathbf{I}$,so that:</p>
<blockquote>
<p>$$ \hat{\mathbf{s}} &#x3D; (\mathbf{H}^\mathbf{*} + \delta \mathbf{I})^{-1} \mathbf{H} \mathbf{r} $$</p>
</blockquote>
<p>where:<br>if $\delta \to 0$,the channel matrix takes over so the $\hat {\mathbf s}$ will be just as the equation<br>if $\delta \to \infty$,the $\mathbf{I}$ takses over and in such cases,$\hat {\mathbf{s}}&#x3D;\mathbf{H^*r}$<br>Completed!</p>
<h3 id="Neumann-Series"><a href="#Neumann-Series" class="headerlink" title="Neumann Series"></a>Neumann Series</h3><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Existing-Methods"><a href="#Existing-Methods" class="headerlink" title="Existing Methods"></a>Existing Methods</h3><p>The loading factor $\alpha$ is defined as $\alpha&#x3D;M&#x2F;N$,with M transmitting and N receiving signals.<br>Existing MMSE and ZF methods exibit near -optimal performances when $\alpha &lt;&lt;1$,while carrying low spectral efficiency.<br>When $\alpha \approx 1$ the spectral efficiency can be attained while losing performance for linear detectors.<br>Later methods like BP and GTA still present not satisfying performance.</p>
<h3 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h3><h4 id="MIMO-System-Model"><a href="#MIMO-System-Model" class="headerlink" title="MIMO System Model"></a>MIMO System Model</h4><p>In this paper,we use real-value system:</p>
<blockquote>
<p>$\mathbf{y}&#x3D;\mathbf{Hx}+\mathbf{n}$</p>
</blockquote>
<p>where $\mathbf y\in \mathbb R^{2N}$,$\mathbf{H}\in \mathbb R^{2N\times 2M}$,$\mathbf{x}\in \Theta^{2M}$.$\mathbf n \sim \mathcal N(0,\sigma_n^2\mathbf I_{2N})$.<br>Assume a uniform prior distribution in this paper,meaning no tendency of a certain symbol.Expressed as $f(\mathbf x)\propto\prod_{i&#x3D;1}^{2M}\mathbb I_{x_i\in \Theta}$,where $\mathbb I_{x_i\in \Theta}$ is the indicator duncition of the constraints of the value of $x_i\in \Theta$<br>If $x_i\in \Theta$,the function&#x3D;1,else the function&#x3D;0.<br>The posterior distribution of $\mathbf x$ has the following representation:<br>$p(\mathbf x|\mathbf y)\propto p(\mathbf y|\mathbf x)f(\mathbf x)\propto \mathcal N(\mathbf y :\mathbf{Hx},\sigma^2_n\mathbf I_{2N})\prod_{i&#x3D;1}^{2M}{\mathbb I_{x_i\in \Theta}}$</p>
<h3 id="EP-MIMO-Detectors"><a href="#EP-MIMO-Detectors" class="headerlink" title="EP  MIMO Detectors"></a>EP  MIMO Detectors</h3><p>EP detector(EPD) enables a Gaussian approximation to be constructed for the posterior distribution of the transmitted symbols based on moment matching.<br>EP requires the explicit calculation of full matrix inversion,resulting in $\mathcal O(M^3)$ computation complexity.<br>The EP-NSA method exhibits great performance with $\alpha&lt;&lt;1$ but suffers performance degradation for larger $\alpha$.<br>The EP-SU method still costs around $\mathcal O(M^3)$.<br>The above methods involve circumventing <strong>matrix inversion</strong>!<br>Therefore,an EPA method is propopsed.Recovering EP performance for$\alpha&lt;1$and approaches EP for $\alpha \geq 1$.</p>
<h3 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h3><p>The remainder of this paper is structured as below. The exact EP approach for massive MIMO detection is introduced in Section II. In Section III, detailed derivation, description and discussion of the proposed algorithm EPA are presented. Section IV shows an alternative derivation of EPA from the viewpoint of EC.Numericalsimulation results of the proposed algorithm and analysis of the computational costs are provided in Section V.Section VI concludes the paper.</p>
<h2 id="Simulation-Analysis"><a href="#Simulation-Analysis" class="headerlink" title="Simulation Analysis"></a>Simulation Analysis</h2><p>Consider i.i.d.(independent and identically distributed)Rayleigh-fading channels with AWGN. </p>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>ZF: zero forcing<br>CHEMP: channel-hardening exploiting message passing<br>GTA: Gaussian tree approximation<br>EP-NSA: EP with Neumann series approximation<br>EP-SU: EP-successive updating<br>EC: expectation consistency<br>i.i.d:independent and identically distributed<br>AWGN: additive white Gaussian noise</p>
]]></content>
      <tags>
        <tag>Reading-Paper</tag>
      </tags>
  </entry>
  <entry>
    <title>Reading:Pattern Recognition and Machine Learning</title>
    <url>/2024/07/17/Reading-Pattern-Recognition-and-Machine-Learning/</url>
    <content><![CDATA[<h2 id="Chapter-8-Graphical-Models"><a href="#Chapter-8-Graphical-Models" class="headerlink" title="Chapter 8:Graphical Models"></a>Chapter 8:Graphical Models</h2><p>In graphical models,there are two parts:</p>
<blockquote>
<p>nodes or vertices<br>links or edges or arcs</p>
</blockquote>
<p>The graphical models mainly divide into two parts</p>
<blockquote>
<p><em>Bayesian networks</em> or <em>directed graphical models</em> </p>
</blockquote>
<blockquote>
<p><em>Markov  random fields</em> or <em>undirected graphical models</em></p>
</blockquote>
<blockquote>
<p>“Directed graphs are useful for expressing causal relationships between<br> random variables, whereas undirected graphs are better suited to expressing soft con<br>straints between random variables. For the purposes of solving inference problems,<br> it is often convenient to convert both directed and undirected graphs into a different<br> representation called a factor graph.”</p>
</blockquote>
<h3 id="Bayesian-Networks"><a href="#Bayesian-Networks" class="headerlink" title="Bayesian Networks"></a>Bayesian Networks</h3><p><em>Multiplication Theorem of Probabilities</em><br>This can be denoted as:</p>
<blockquote>
<p>$p(a,b)&#x3D;p(a|b)p(b)$</p>
</blockquote>
<p>which originates from the <em>conditional probability</em></p>
<blockquote>
<p>$p(A|B)&#x3D;\frac{p(AB)}{p(B)}$</p>
</blockquote>
<p>For this relationship $P(a,b,c)$,we can take it separately as:</p>
<blockquote>
<p>$P(a,b,c)&#x3D;P(c|a,b)P(a,b)&#x3D;P(c|a,b)P(b|a)P(a)$</p>
</blockquote>
<p>This can be represented by a graph:<br><img src="/images/graph81.png" alt="图片" title="Graph Denoting a,b,c"><br>Now let’s consider a more complicated scenario,there are N numbers in a column vector, polynomial coefficients $\mathbf{w}$.The input data $\mathbf{x}&#x3D;(x_1,…,x_n)^T$ and the observed data$\mathbf{t}&#x3D;(t_1,…,t_n)^T$,with a Gaussian noises variance $\sigma^2$ and a hyperparameter $\alpha$ representing the precision of the Gaussian prior over $\mathbf{w}$.<br>The joint distribution is given by the product of the prior $p(\mathbf{w})$ and $N$ conditional distributions $p(t_n|\mathbf{w})$ for $n&#x3D;1,2,…,N$.</p>
<blockquote>
<p>$p(\mathbf{t,w})&#x3D;p(\mathbf{w})\displaystyle \prod_{n&#x3D;1}^{N}{p(t_n|\mathbf{w})}$</p>
</blockquote>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>PDF: probability density function</p>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>First Glance at Covariance Matrix</title>
    <url>/2024/07/18/First-Glance-at-Covariance-Matrix/</url>
    <content><![CDATA[<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Let $X$ be a $N\times 1$ random column vector.The <em>covariance matrix</em> of $X$ is denoted as $Var[X]$ ,and defined as</p>
<blockquote>
<p>$Var[X]&#x3D;E[(X-E(X))(X-E[X])^T]$</p>
</blockquote>
<p>where $E(\cdot)$ denoted the expectation of $\cdot$.<br>In the equation above,we can transform the dot product form into square form,in other words:</p>
<blockquote>
<p>$E[(X-E(X))(X-E[X])^T]&#x3D;E[(X-E(X))^2]$</p>
</blockquote>
<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Suppose $x$ is a $2\times 1$ vector,with $X_1,X_2$ as its components.<br>Let</p>
<blockquote>
<p>$Var[X_1]&#x3D;2$<br>$Var[X_2]&#x3D;4$<br>$Cov[X_1,X_2]&#x3D;1$</p>
</blockquote>
<p>Noticably,</p>
<blockquote>
<p>$Var[X]&#x3D;\begin{bmatrix}<br>Var[X_1] &amp;Cov[X_1,X_2]\\<br>Cov[X_2,X_1] &amp;Var[X_2]<br>\end{bmatrix}\<br>&#x3D;\begin{bmatrix}<br>2 &amp;1\\<br>1 &amp;4<br>\end{bmatrix}<br>$</p>
</blockquote>
<p>The covariance matrix can also be counted by using the equivalent formula below:</p>
<blockquote>
<p>$Var[X]&#x3D;E[XX^T]-E[X]E[X]^T$</p>
</blockquote>
<p>Also,we have such corollaries below:</p>
<blockquote>
<p>$Var[a+X]&#x3D;Var[X]$</p>
</blockquote>
<blockquote>
<p>$Var[bX]&#x3D;bVar[X]b^T$</p>
</blockquote>
<blockquote>
<p>$Var[X]^T&#x3D;Var[X]$</p>
</blockquote>
<h2 id="Covariance-between-Linear-Transformations"><a href="#Covariance-between-Linear-Transformations" class="headerlink" title="Covariance between Linear Transformations"></a>Covariance between Linear Transformations</h2><p>Let $a$ and $b$ be two constant $1\times K$ vectors and $X$ a $K\times 1$ random vector.The covariance between two linear transformations $aX$ and $bX$ is</p>
<blockquote>
<p>$Cov[aX,bX]&#x3D;aVar[X]b^T$</p>
</blockquote>
<h2 id="Cross-Covariance"><a href="#Cross-Covariance" class="headerlink" title="Cross-Covariance"></a>Cross-Covariance</h2><p>For two random vectors,what will happen?<br>Let $X$ be a $K\times 1$ vector and $Y$ be a $L\times 1$ vector(random).The covariance matrix between $X,Y$ is denoted by $Cov[X,Y]$</p>
<blockquote>
<p>$Cov[X,Y]&#x3D;E[(X-E(X)(Y-E(Y))^T]$</p>
</blockquote>
<p>resulting in a $K\times L$ matrix.<br>Apparently,$Cov[X,Y]\neq Cov[Y,X]$,but $Cov[X,Y]&#x3D; (Cov[Y,X])^T$<br>What’s worth noting is that the Covariance Matrix bears good <em>linearity</em>!</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p><em>Taboga, Marco (2021). “Covariance matrix”, Lectures on probability theory and mathematical statistics. Kindle Direct Publishing. Online appendix. <a href="https://www.statlect.com/fundamentals-of-probability/covariance-matrix">https://www.statlect.com/fundamentals-of-probability/covariance-matrix</a>.</em></p>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Analysis:How to Form a Paper</title>
    <url>/2024/07/19/Analysis-How-to-Form-a-Paper/</url>
    <content><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><strong>Abstract</strong></h2><p>$\quad$ Recent environment encourage students to create their own works or even present paper in the undergraduate period.But for most of the first and second year student,they find themselves hard to acquire the basic and needed skills and knowledge to structurize a formal paper with great proficiency.In order to prevent students from being tortured and to prevent peer viewers and mentors from relentlessly modifying and correcting paper,this work is proposed.The aim of this paper is to propose the conventional formula of how to c</p>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Variance and Covariance</title>
    <url>/2024/07/22/Variance-and-Covariance/</url>
    <content><![CDATA[<h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>The <em>variance</em> of a random variable $X$ can be defined as:</p>
<blockquote>
<p>If $\mathbb E(X)&#x3D;\mu$</p>
</blockquote>
<blockquote>
<p>$var(X)&#x3D;\mathbb E((x-\mu)^2)$</p>
</blockquote>
<p>For its <em>standard deviation</em>,soometimes denotes as $sd(X)$,is defined as</p>
<blockquote>
<p>$sd(X)&#x3D;\sqrt{var(X)}$</p>
</blockquote>
<p>The variance has the following properties:</p>
<blockquote>
<p>$var(X+C)&#x3D;var(X)$<br>$var(bX)&#x3D;b^2var(X)$</p>
</blockquote>
<p>Similarly,in the covariance matrix chapter,the <em>covariance matrix</em> has near-the-same properties.</p>
<h3 id="The-Tchebychev-inequality"><a href="#The-Tchebychev-inequality" class="headerlink" title="The Tchebychev inequality"></a>The Tchebychev inequality</h3><blockquote>
<p>$\mathbb P{(|X-\mu|)&gt;\epsilon}\leq \frac{var(X)}{\epsilon^2}$</p>
</blockquote>
<p>where $\mu$ is the expectation of $X$,and $\epsilon&gt;0$<br>The variance can be sometimes canculated in another way:</p>
<blockquote>
<p>$var(X)&#x3D;\mathbb E(X^2)-\mathbb E(X)^2$</p>
</blockquote>
<h2 id="Covariance"><a href="#Covariance" class="headerlink" title="Covariance"></a>Covariance</h2><p>The <em>covariance</em> of two random variables $X$ and $Y$ is defined as:</p>
<blockquote>
<p>$cov(X,Y)&#x3D;\mathbb E{(X-\mathbb E(X))(Y-\mathbb E(Y))}$</p>
</blockquote>
<p>for a more complicated expression with $a,b,c,d$ as constants and $U,V,X,Y$ as random variables,we denote:</p>
<blockquote>
<p>$cov(aU+bV,cX+dY)&#x3D;accov(U,X)+bccov(V,X)+adcov(U,Y)+bdcov(V,Y)$</p>
</blockquote>
<h2 id="Correlation"><a href="#Correlation" class="headerlink" title="Correlation"></a>Correlation</h2><p>The <em>correlation</em> between two variables $Y$ and $Z$ can be denoted as:</p>
<blockquote>
<p>corr(Y,Z)&#x3D;\frac{cov(Y,Z)}{\sqrt{var(Y)var(Z)}}</p>
</blockquote>
<h2 id="Standardized-Variables"><a href="#Standardized-Variables" class="headerlink" title="Standardized Variables"></a>Standardized Variables</h2><p>To standardize between different variables,we need to find a way:<em>standardized variables</em>.For two variables $Y$ and $Z$,with their mean and variance $\mu _Y$ $\sigma^2_Y$ $\mu _Z$ $\sigma^2_Z$.We standardize each with the following:</p>
<blockquote>
<p>$Y’&#x3D;\frac{Y-\mu_Y}{\sigma_Y}$<br>$Z’&#x3D;\frac{Z-\mu_Z}{\sigma_Z}$</p>
</blockquote>
<p>In this way,the mean of $Y’,Z’$ is 0 and the variance of them is 1,and</p>
<blockquote>
<p>$corr(Y,Z)&#x3D;cov(Y’,Z’)&#x3D;\mathbb E(Y’Z’)$</p>
</blockquote>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Gaussian Distribution</title>
    <url>/2024/07/22/Gaussian-Distribution/</url>
    <content><![CDATA[<h2 id="Why-Gaussian-Distribution–Central-Limit-Theorem"><a href="#Why-Gaussian-Distribution–Central-Limit-Theorem" class="headerlink" title="Why Gaussian Distribution–Central Limit Theorem"></a>Why Gaussian Distribution–Central Limit Theorem</h2><h3 id="Population-and-Sample"><a href="#Population-and-Sample" class="headerlink" title="Population and Sample"></a>Population and Sample</h3><p>The <em>population</em> refers to the whole set of individuals that we want to study,and the <em>sample</em> refers to the subset that we choose from the population to study(in respect for the fact that we can’t choose the whole population to study in a row)</p>
<h3 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h3><p>The Central Limit Theorem (CLT) is a fundamental theorem in probability theory and statistics that states that the distribution of the sum (or average) of a large number of independent, identically distributed variables will be approximately normal, regardless of the underlying distribution.</p>
<p>More specifically, if we take a sample of $n$ observations from any population, calculate the sample’s mean, and repeat that process a large number of times, the distribution of those sample means will be normally distributed. This distribution will have the same mean as the population from which the samples were drawn, and its standard deviation (also known as the standard error) will be equal to the standard deviation of the population divided by the square root of the sample size.</p>
<h2 id="Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution"><a href="#Single-Variable-Univariate-Gaussian-Distribution-Normal-Distribution" class="headerlink" title="Single Variable(Univariate) Gaussian Distribution(Normal Distribution)"></a>Single Variable(Univariate) Gaussian Distribution(Normal Distribution)</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>For a random variable $x$,if its <em>probability density function</em> is:</p>
<blockquote>
<p>$f(x)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$</p>
</blockquote>
<p>where $\mu$ and $\sigma$ are the mean and variance of $x$.Then we can say the variable follows the normal distribution.<br>If the variable $x$ follows the Gaussian distribution,we can denote this kind of distribution as:</p>
<blockquote>
<p>$x\sim N(\mu,\sigma^2)$</p>
</blockquote>
<h2 id="Multivariate-Normal-Distribution"><a href="#Multivariate-Normal-Distribution" class="headerlink" title="Multivariate Normal Distribution"></a>Multivariate Normal Distribution</h2><p>For a mutivariable $n\times 1$ vector $X$ distributed following the multivariate distribution,with population mean vector $\mathbf{\mu}$ and a variance-covariance matrix $\Sigma$,the PDF(possibbility density function) can be described as:</p>
<blockquote>
<p>$\phi(X)&#x3D;\frac{|\Sigma|^{\frac{-1}{2}}}{(2\pi)^{\frac{n}{2}}}\exp{\frac{-1}{2}(X-\mathbf \mu)^T\Sigma^{-1}(X-\mathbf\mu) }$</p>
</blockquote>
<p>If $n&#x3D;2$ ,then we get the <em>bivariate normal distribution</em>,resulting in a bell-shaped curve in three dimensions.</p>
<h3 id="Mahalanobis-Distance"><a href="#Mahalanobis-Distance" class="headerlink" title="Mahalanobis Distance"></a>Mahalanobis Distance</h3><p>Noticably,the mahalanobis distance is a distance!In the univariate case,if it is greater than zero,then the variable is larger than mean,vice versa.<br>In the multivariate case,since the covariance matrix $\Sigma$ is always positive definite,the distance is always greater than 0!</p>
<h4 id="In-Univariate-Normal-Distribution"><a href="#In-Univariate-Normal-Distribution" class="headerlink" title="In Univariate Normal Distribution"></a>In Univariate Normal Distribution</h4><p>This distance can be measured as:</p>
<blockquote>
<p>$d&#x3D;\frac{x-\mu}{\sigma}$</p>
</blockquote>
<h4 id="In-Multivariate-Normal-Distribution"><a href="#In-Multivariate-Normal-Distribution" class="headerlink" title="In Multivariate Normal Distribution"></a>In Multivariate Normal Distribution</h4><p>Also,this distance can be measured as:</p>
<blockquote>
<p>$d&#x3D;\sqrt{(X-\mu)^T\Sigma^{-1}(X-\mu)}$</p>
</blockquote>
<h3 id="Integral-Property"><a href="#Integral-Property" class="headerlink" title="Integral Property"></a>Integral Property</h3><p>For univariate normal distribution:</p>
<blockquote>
<p>$\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}{\exp{\frac{-1}{2\sigma^2}(x-\mu)^2}}&#x3D;1$</p>
</blockquote>
<p>For multivariate normal distribution:</p>
<blockquote>
<p>$\frac{1}{2\pi^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}…\int_{-\infty}^{\infty}\exp{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)dx_1dx_2…dx_n}&#x3D;1$</p>
</blockquote>
<h2 id="Diagonal-Covariance-Matrix-Product-Gaussian"><a href="#Diagonal-Covariance-Matrix-Product-Gaussian" class="headerlink" title="Diagonal Covariance Matrix-&gt;Product Gaussian"></a>Diagonal Covariance Matrix-&gt;Product Gaussian</h2><p>Let’s imagine a diagonal covariance matrix $\Sigma$,OK,cool!This means the variables are independent!<br>Let’s take a $2\times 2$ covariance matrix for example:<br>In this column,we define $X&#x3D;(x_1,x_2)$,the variance matrix $\Sigma &#x3D; [\begin{matrix}<br>\sigma_1^2 &amp; 0 \\<br>0 &amp; \sigma_2^2<br>\end{matrix} ]$<br>and the mean vector $\mu&#x3D;(\mu_1,\mu_2)$.<br>WE can easily compute the inverse matrix $\Sigma^{-1}&#x3D;[\begin{matrix}<br>\frac{1}{\sigma_1^2} &amp; 0 \\<br>0 &amp; \frac{1}{\sigma_2^2}<br>\end{matrix} ]$<br>In this case,we can find the PDF as:</p>
<blockquote>
<p>$p(x;\mu,\Sigma)&#x3D;\frac{1}{2\pi\sigma_1\sigma_2}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}$</p>
</blockquote>
<p>Further computing it,we get:</p>
<blockquote>
<p>$p(x;\mu,\Sigma)&#x3D;\frac{1}{\sqrt{2\pi}\sigma_1}\exp{(-\frac{1}{2\sigma_1^2}(x_1-\mu_1)^2)}\frac{1}{\sqrt{2\pi}\sigma_2}\exp{(-\frac{1}{2\sigma_2^2}(x_2-\mu_2)^2)}$</p>
</blockquote>
<p>Further corollary shows for a n-dimensional variable vector $x$,we have:<br>$p(x;\mu,\Sigma)&#x3D;\prod_{i&#x3D;1}^{n}\frac{1}{\sqrt{2\pi}\sigma_{i}}\exp{(-\frac{1}{2\sigma_i^2}(x_i-\mu_i)^2)}$</p>
<h2 id="Ellipse-Where-Does-This-Thing-Come-From"><a href="#Ellipse-Where-Does-This-Thing-Come-From" class="headerlink" title="Ellipse:Where Does This Thing Come From??"></a>Ellipse:Where Does This Thing Come From??</h2><h3 id="Isocontour-Hooray-Geography"><a href="#Isocontour-Hooray-Geography" class="headerlink" title="Isocontour(Hooray Geography!)"></a>Isocontour(Hooray Geography!)</h3><p>Let us take the same definition above and consider this:</p>
<blockquote>
<p>Let $c&#x3D;\frac{1}{2\pi\sigma_1\sigma_2}\exp(-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2)$<br>$2\pi c\sigma_1\sigma_2&#x3D;\exp(-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2)$<br>$\log(2\pi\sigma_1\sigma_2)&#x3D;-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_1})^2-\frac{1}{2}(\frac{x_2-\mu_2}{\sigma_2})^2$<br>$1&#x3D;\frac{(x_1-\mu_1)^2}{2\sigma_1^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}+\frac{(x_2-\mu_2)^2}{2\sigma_2^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$</p>
</blockquote>
<p>Define:</p>
<blockquote>
<p>$r_1&#x3D;\sqrt{2\sigma_1^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$<br>$r_2&#x3D;\sqrt{2\sigma_2^2\log(\frac{1}{2\pi c\sigma_1\sigma_2})}$</p>
</blockquote>
<p>An ellipse occurs!</p>
<h3 id="Non-diagonal-Higher-Dimensional-–-Still-Symmetric"><a href="#Non-diagonal-Higher-Dimensional-–-Still-Symmetric" class="headerlink" title="Non-diagonal&#x2F;Higher Dimensional – Still Symmetric"></a>Non-diagonal&#x2F;Higher Dimensional – Still Symmetric</h3><p>For a two dimensional covariance matrix,this turns to a non-axis-aligned ellipse.<br>For a higher dimenstional one,say in the $n$-dimensional case,we get a high dimensional ellipsoids in $\mathbf R^n$</p>
<h2 id="Linear-Transformation-This-Is-a-Matrix"><a href="#Linear-Transformation-This-Is-a-Matrix" class="headerlink" title="Linear Transformation: This Is a Matrix!"></a>Linear Transformation: This Is a Matrix!</h2><p>Here is a theorem saying:</p>
<blockquote>
<p>Let $X\sim \mathcal N(\mu,\Sigma)$ for some $\mu\in \mathbf R^n$ and $\Sigma\in \mathbf S_{++}^n$.Then there exists a matrix $B\in R^{n\times n}$ such that if $Z&#x3D;B^{-1}(X-\mu)$,then $Z\sim \mathcal N(0,I)$</p>
</blockquote>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>FirstArticle</title>
    <url>/2024/07/09/Unused/Article/</url>
    <content><![CDATA[<h2 id="这是一个简单的测试文章"><a href="#这是一个简单的测试文章" class="headerlink" title="这是一个简单的测试文章"></a>这是一个简单的测试文章</h2><blockquote>
<p><em>祈求在路上没任何的阻碍，令愉快旅程变悲哀</em><br><strong>–杨千嬅</strong></p>
</blockquote>
<h3 id="这是一个小标题-小标题呀小标题"><a href="#这是一个小标题-小标题呀小标题" class="headerlink" title="这是一个小标题 小标题呀小标题"></a>这是一个小标题 小标题呀小标题</h3><p>这是一个公式，能不能运行捏？</p>
<blockquote>
<p>$\sum_{n&#x3D;1}^{\infty}log_{2}(n)$</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>MMSE and Relative Methods</title>
    <url>/2024/07/23/MMSE-and-Relative-Methods/</url>
    <content><![CDATA[<h2 id="MMSE-Estimation"><a href="#MMSE-Estimation" class="headerlink" title="MMSE Estimation"></a>MMSE Estimation</h2><h3 id="Estimation-with-PDF"><a href="#Estimation-with-PDF" class="headerlink" title="Estimation with PDF"></a>Estimation with PDF</h3><p>For a random variable $x:\Omega \to \mathbb R^n$ with its PDF $p^x$<br>We can <em>predict</em> or <em>estimate</em> the outcome:</p>
<blockquote>
<p>Given <em>cost function</em> $c$: $\mathbb<br>R^n\times\mathbb R^n\to R$<br>Pick <em>estimate</em> $\hat x$ to minimize $\mathbf Ex(x,\hat x)$</p>
</blockquote>
<h4 id="The-Cost-Function"><a href="#The-Cost-Function" class="headerlink" title="The Cost Function"></a>The Cost Function</h4><blockquote>
<p>$c(x,\hat x)&#x3D;||x-\hat x||^2$</p>
</blockquote>
<h4 id="The-Mean-Square-Error"><a href="#The-Mean-Square-Error" class="headerlink" title="The Mean Square Error"></a>The Mean Square Error</h4><p>The <em>mean square error</em>(MSE) is:</p>
<blockquote>
<p>$\mathbf E(||x-\hat x||^2)&#x3D;\int ||x-\hat x||^2p^x(x)dx$</p>
</blockquote>
<p>The PDF in this equation can be interpreted as a kind of special weight function!</p>
<h4 id="The-Minimum-Mean-Square-Error"><a href="#The-Minimum-Mean-Square-Error" class="headerlink" title="The Minimum Mean-Square Error"></a>The Minimum Mean-Square Error</h4><p>The <em>minimum mean-square error</em> is defined as:</p>
<blockquote>
<p>$\displaystyle \min_{\hat x}\mathbf  E(||x-\hat x||^2)$</p>
</blockquote>
<p>We can derive from that formula:</p>
<blockquote>
<p>$\displaystyle \min_{\hat x}\mathbf  E(||x-\hat x||^2)\\&#x3D;\displaystyle \min_{\hat x}\mathbf  E((x-\hat x)^T(x-\hat x))\\&#x3D;\mathbf E||x||^2-2\hat x^T\mathbf Ex+\hat x^T \hat x$</p>
</blockquote>
<p>Take the derivative of it,we get:</p>
<blockquote>
<p>$\hat x_{mmse}&#x3D;\mathbf Ex$</p>
</blockquote>
<p>The mean square error of it is:</p>
<blockquote>
<p>$\mathbf E(||x-\hat x_{mmse}||^2)&#x3D;\mathbf{trace cov}(x)$</p>
</blockquote>
<h4 id="The-Mean-Variance-Decomposition"><a href="#The-Mean-Variance-Decomposition" class="headerlink" title="The Mean-Variance Decomposition"></a>The Mean-Variance Decomposition</h4><p>The MVD can be interpreted as:</p>
<blockquote>
<p>$\mathbf E(||z||^2)&#x3D;\mathbf E(||z-\mathbf Ez||^2)+||\mathbf Ez||^2$</p>
</blockquote>
<p>Applying this to the $z&#x3D;x-\hat x$</p>
<blockquote>
<p>$\mathbf E(||x-\hat x||^2)&#x3D;\mathbf E(||x-\mathbf Ex||^2)+||\hat x-\mathbf Ex||^2$</p>
</blockquote>
<p>The first term is the <em>variance</em> of $x$<br>The second term is the <em>bias</em> of $x$</p>
<h3 id="The-Estimation-Problem"><a href="#The-Estimation-Problem" class="headerlink" title="The Estimation Problem"></a>The Estimation Problem</h3><p>$x,y$ are random variables,with joint PDF $p(x,y)$<br>We have already measured the variable $y&#x3D;y_{meas}$<br>Now we want to find the MMSE of $x$ given $y&#x3D;y_{meas}$<br>Note that the <em>estimator</em> stands for a function $\phi:\mathbb R^m\to \mathbb R^n$<br>We define $\hat x_{est}&#x3D;\phi(y_{meas})$<br>How can we find the function $\phi$ to minimize the cost function?</p>
<blockquote>
<p>$J&#x3D;\mathbf E(||\phi(y)-x||^2)$</p>
</blockquote>
<p>We define the <em>marginal PDF</em> of $y$ as:</p>
<blockquote>
<p>$p^y(y)&#x3D;\int p(x,y)dx$ </p>
<blockquote>
<p>Wow!We can easily notice that this is a continuous form of the marginal function!(See in the factor graph section!).What a surprise!</p>
</blockquote>
</blockquote>
<p>And the <em>conditioned pdf</em> of $y$:</p>
<blockquote>
<p>$p^{|y}(y)&#x3D;\frac{p(x,y)}{p^y(y)}$</p>
</blockquote>
<h3 id="The-MMSE-Estimator"><a href="#The-MMSE-Estimator" class="headerlink" title="The MMSE Estimator"></a>The MMSE Estimator</h3><p>The <em>mean-square-error conditioned on</em> $y$ is $e_{cond}(y)$,given by:</p>
<blockquote>
<p>$e_{cond}(y)&#x3D;\int||\phi(y)-x||^2p^{|y}(x,y)dx$</p>
</blockquote>
<p>Then the mean square error $J$ is given by:</p>
<blockquote>
<p>$J&#x3D;\mathbf E(e_{cond}(y))$</p>
</blockquote>
<p>Because:</p>
<blockquote>
<p>$J&#x3D;\int\int ||\phi(y)-x||^2p(x,y)dxdy\\&#x3D;\int p^y(y)e_{cond}(y)dy$</p>
</blockquote>
<h3 id="The-affine-transformation-in-signal-processing"><a href="#The-affine-transformation-in-signal-processing" class="headerlink" title="The affine transformation(in signal processing)"></a>The affine transformation(in signal processing)</h3><p>For a signal $\mathbf y$ received and a certain <em>channel matrix</em> $\mathbf H$,we have:</p>
<blockquote>
<p>$\mathbf {y&#x3D;Hs+n}$</p>
</blockquote>
<p>where we suppose $\mathbf{y\in R^{N_r},s\in R^{N_t}},H\in R^{N_r\times N_t}$<br>The expectation of $\mathbf y$ is:</p>
<blockquote>
<p>$\mathbf {Ey&#x3D;E(Hs+n)&#x3D;HE(\mathbf s)}&#x3D;H\mu$</p>
</blockquote>
<p>The covariance is:</p>
<blockquote>
<p>$\mathbf {\Sigma_y &#x3D;E(y-\hat y)(y-\hat y)^T&#x3D;EH(s- \hat s)(s-\hat s)^TH^T}\\$<br>$&#x3D;\mathbf{H\Sigma_sH^T+\Sigma_n}$</p>
</blockquote>
<p>which denotes the <em>signal covariance</em> and <em>noise covariance</em></p>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Continuous RV distributions</title>
    <url>/2024/07/25/Continuous-RV-distributions/</url>
    <content><![CDATA[<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>In recent days of browsing the Internet for relevant learning materials,I find it good to have a better understanding of possibilities by looking at some basic concepts!Therefore,I come up with this chapter:<em>Continuous RV distributions</em>,where RV stands for random variables.</p>
<h2 id="Continuous-Random-Variables"><a href="#Continuous-Random-Variables" class="headerlink" title="Continuous Random Variables"></a>Continuous Random Variables</h2><p>Every continuous random variable has a probability density function (PDF), instead of a probability mass function (PMF),where the limit in calculus takes place,showing in a given section,the possibility of a given event to happen is 0.</p>
<h3 id="PDF-and-CDF"><a href="#PDF-and-CDF" class="headerlink" title="PDF and CDF"></a>PDF and CDF</h3><p>PDF:<br>define $f(x)$ as the possibility density function of $x$ so that:</p>
<blockquote>
<p>$\int_{-\infty}^{\infty}f(x)dx&#x3D;1$<br>$\mathbb P(x\approx q)&#x3D;\mathbb P(q-\frac{\epsilon}{2}\leq X\leq q+\frac{\epsilon}{2})\approx \epsilon f_x(q)$</p>
</blockquote>
<p>CDF:<br>define $F(x)$ as the cumulative distribution function so that:</p>
<blockquote>
<p>$F_x(w)&#x3D;\mathbb P(x\leq w)&#x3D;\int _{-\infty}^{w}{f(x)dx}$</p>
</blockquote>
<p>Noticably,PDF is the derivative of CDF.</p>
<h3 id="Expectation-and-Variance"><a href="#Expectation-and-Variance" class="headerlink" title="Expectation and Variance"></a>Expectation and Variance</h3><p>The expectation of a continuous RV can be counted as follows:</p>
<blockquote>
<p>$\mathbb E(x)&#x3D;\int_{-\infty}^{\infty}{tf_x(t)dt}$</p>
</blockquote>
<p>The variance of a RV can be counted as follows:</p>
<blockquote>
<p>$Var(x)&#x3D;\mathbb E(x^2)-\mathbb E^2(x)$</p>
</blockquote>
<p>or also:</p>
<blockquote>
<p>$Var(x)&#x3D;\int_{-\infty}^{\infty}{(t-\mathbb E(x))f_x(t)dt}$</p>
</blockquote>
<h3 id="Exponential-Family"><a href="#Exponential-Family" class="headerlink" title="Exponential Family"></a>Exponential Family</h3><p>$X\sim Exp(\lambda)$,if X follows the pdf:</p>
<blockquote>
<p>$f_X(x)&#x3D;\begin{cases}\lambda e^{-\lambda x},x\geq 0\\0,otherwise\end{cases}$</p>
</blockquote>
<p>Calculating this,we know:</p>
<blockquote>
<p>$\mathbb E(X)&#x3D;\frac{1}{\lambda},Var(X)&#x3D;\frac{1}{\lambda^2}$</p>
</blockquote>
<p>The cdf of it is:</p>
<blockquote>
<p>$F_X(x)&#x3D;\begin{cases}1-e^{-\lambda x},x\geq 0\\0,otherwise\end{cases}$</p>
</blockquote>
<h3 id="The-Gamma-Gamma-RV"><a href="#The-Gamma-Gamma-RV" class="headerlink" title="The Gamma($\Gamma$) RV"></a>The Gamma($\Gamma$) RV</h3><p>$X\sim Gamma(r,\lambda)$ if and only if $X$ has the following pdf:</p>
<blockquote>
<p>$f_X(x)&#x3D;\begin{cases}\frac{\lambda^r}{(r-1)!}{x^{r-1}e^{-\lambda x}},x&gt;0\\0,otherwise\end{cases}$</p>
</blockquote>
<p>where:</p>
<blockquote>
<p>$\mathbb E(X)&#x3D;\frac{r}{\lambda},Var(X)&#x3D;\frac{r}{\lambda^2}$</p>
</blockquote>
]]></content>
      <tags>
        <tag>learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Rotation Matrix with MATLAB Code</title>
    <url>/2024/10/09/code%E2%80%9D/</url>
    <content><![CDATA[<h2 id="What-we-are-up-to-do-here"><a href="#What-we-are-up-to-do-here" class="headerlink" title="What we are up to do here"></a>What we are up to do here</h2><p><strong>In this blog,we will visualize how rotation matrix works</strong><br>Code first:<br><figure class="highlight plaintext"><figcaption><span>[Code to demonstrate rotation]</span></figcaption><table><tr><td class="code"><pre><span class="line">lb=-pi;</span><br><span class="line">ub=pi;</span><br><span class="line">theta=lb+(ub-lb)*rand(1);</span><br><span class="line">x=[1,1];</span><br><span class="line">%figure;</span><br><span class="line">%quiver(0,0,x(1),x(2));</span><br><span class="line">R=[cos(theta),sin(theta);sin(-theta),cos(theta)];</span><br><span class="line">x_trans=R*x&#x27;;</span><br><span class="line">disp((theta*360)/(2*pi));</span><br><span class="line">figure;</span><br><span class="line">quiver(0,0,x_trans(1),x_trans(2));</span><br><span class="line">title(&#x27;transformed angle:&#x27;,(theta*360)/(2*pi));</span><br></pre></td></tr></table></figure><br>With the code,we can plot figures of rotation,as follows:<br><strong><em>Demonstrate original vector</em></strong><br><img src="/images/OriginalVec.jpg" alt="图片" title="Original Vector"><br><strong><em>Demonstrate rotated vector</em></strong><br><img src="/images/RotatedVec.jpg" alt="图片" title="Original Vector"><br>By setting the rotation angle $\theta$ between $[-\pi,\pi]$,we can see in the example above,the vector is rotated anti-clockwise with the angle demonstrated.</p>
]]></content>
  </entry>
</search>
